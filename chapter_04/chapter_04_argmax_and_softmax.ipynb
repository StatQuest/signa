{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34006ee1-51e5-4a48-82de-021a71e1201a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# The StatQuest Illustrated Guide to Neural Networks and AI\n",
    "## Chapters 4 and 5 - ArgMax, SoftMax, and Cross Entropy!!!\n",
    "\n",
    "Copyright 2024, Joshua Starmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0aedd-9400-4ce1-8885-d800f997cb6b",
   "metadata": {},
   "source": [
    "---- \n",
    "\n",
    "In this tutorial, we will use **[PyTorch](https://pytorch.org/) + [Lightning](https://www.lightning.ai/)** to create and optimize a simple neural network with multiple inputs and outputs that uses the `SoftMax()` function during training, the the one in the picture shown below...\n",
    "\n",
    "<img src=\"./images/chapter_4_softmax.png\" alt=\"a neural network with multiple inputs and outputs and softmax\" style=\"width: 800px;\">\n",
    "\n",
    " and the `ArgMax()` function during inference.\n",
    "\n",
    "<img src=\"./images/chapter_4_argmax.png\" alt=\"a neural network with multiple inputs and outputs and argmax\" style=\"width: 800px;\">\n",
    "\n",
    "\n",
    "In this tutorial, you will...\n",
    "\n",
    "- **[Import and Format Data and then Build a DataLoader From Scratch](#data)**\n",
    "- **[Build a Neural Network with Multiple Inputs and Outputs and the SoftMax() Function](#build)**\n",
    "- **[Train a Neural Network with Multiple Inputs and Outputs and the SoftMax() Function](#train)**\n",
    "- **[Make Predictions with New Data Using the Trained Model and the ArgMax() Function](#predict)**\n",
    "\n",
    "#### NOTE:\n",
    "This tutorial assumes that you already know the basics of coding in **Python** and have read the first four chapters in **The StatQuest Illustrated Guide to Neural Networks and AI.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3669a0-284c-48f7-9c73-19812e2a8fac",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4feccd-8472-4bb2-bc56-ea41e527d714",
   "metadata": {},
   "source": [
    "# Import the modules that will do all the work\n",
    "\n",
    "The very first thing we need to do is load a bunch of Python modules. Python itself is just a basic programming language. These modules give us extra functionality to create and train a Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aee3f72-35c0-4e88-9974-1386de1eee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# %%capture prevents this cell from printing a ton of STDERR stuff to the screen\n",
    "\n",
    "## First, check to see if lightning is installed, if not, install it.\n",
    "import pip\n",
    "try:\n",
    "  __import__(\"lightning\")\n",
    "except ImportError:\n",
    "  pip.main(['install', \"lightning\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6b9647-24a8-4c13-9665-a036d9a8e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # torch will allow us to create tensors.\n",
    "import torch.nn as nn # torch.nn allows us to create a neural network.\n",
    "import torch.nn.functional as F # nn.functional give us access to the activation and loss functions.\n",
    "from torch.optim import Adam # optim contains many optimizers. This time we're using Adam\n",
    "\n",
    "import lightning as L # lightning has tons of cool tools that make neural networks easier\n",
    "from torch.utils.data import TensorDataset, DataLoader # these are needed for the training data\n",
    "\n",
    "import pandas as pd # We'll use pandas to read in the data and normalize it\n",
    "from sklearn.model_selection import train_test_split # We'll use this to create training and testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e66ff-db47-4057-9f50-7c9576ec58ee",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23672f4-c30e-42de-9d35-5fa6b9bfd0f5",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "# Import and Format data and then Build a DataLoader From Scratch.\n",
    "\n",
    "Once we have the Python modules imported, now we need to import the data that we will use to train and test our neural network. Specifically, we're going to use the **[Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set)**, which we will import from a comma-separated (CSV) text file so that we can learn how to build a DataLoader from scratch.\n",
    "\n",
    "The **[Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set)** is a classic dataset originally made famous by Rondal Fisher in 1936, and has since been used countless times to demonstrate the effectiveness of various classification algorithms. The dataset consists of 150 samples total, 50 for each of 3 species of Iris, **Setosa**, **Versicolor**, and **Virginica**. Each row in the dataset contains measurements for 4 variables: **[petal](https://en.wikipedia.org/wiki/Petal)** width and length and **[sepal](https://en.wikipedia.org/wiki/Sepal)** width and length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f49de94-6dfe-43d1-a97b-44b07b31b1c4",
   "metadata": {},
   "source": [
    "**NOTE:** The data file we are going import, `iris.txt`, was originally downloaded from the **[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/)**, which has a lot of great datasets that you can practice building Neural Networks (or any other machine learning algorithm) with. When you download the datasets from UCI, you get one file that has the data and another file that describes the data, including providing us with the names of each variable, or column, in the dataset. If you'd like to see the original data files, you can find them **[here](https://archive.ics.uci.edu/dataset/53/iris)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b5096-a942-4097-800a-ea38b24c5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll read in the dataset with the pandas function read_table()\n",
    "## read_table() can read in various text files including, comma-separated and tab-delimted.\n",
    "df = pd.read_table('iris.txt', sep=\",\", header=None)\n",
    "## NOTE: If the data were tab-delimted, we would set sep=\"\\t\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb56234a-7c1d-4f62-a655-e3d722ba50c7",
   "metadata": {},
   "source": [
    "Now, in theory, we have loaded the data into a DataFrame called `df`, but it's always a good idea to make sure this worked as expected. So, we'll print out the first handful of rows in the dataset with the `head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f040fe4-dc2c-4258-a438-38d7d975da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## print out the first handful of rows using the head() method\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b97bc9-edc4-4915-a498-7a1bb239cd17",
   "metadata": {},
   "source": [
    "When we print out the first few rows of our new DataFrame, `df`, the first thing we see is that the columns are not named. In theory, it's fine to have unnamed columns (and just have numbers), but it makes the data hard to look at, so let's add the column names to `df`. To name each column, we simply assign a list of column names to `columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2226d80c-defe-4dd7-aa8f-6657346d5279",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To name each column, we assign a list of column names to `columns`\n",
    "df.columns = [\"sepal_length\",\n",
    "              \"sepal_width\", \n",
    "              \"petal_length\", \n",
    "              \"petal_width\", \n",
    "              \"class\"]\n",
    "\n",
    "## To verify we did that correctly, let's print out the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12738def-82bb-4bc1-b917-d1cf9388d5f8",
   "metadata": {},
   "source": [
    "Hooray! Now that we can look at our DataFrame without getting a headache, let's see how big this dataset is and figure out how many different iris species we will have to train our neural network to predict. First, let's see how many rows and columns are in the dataset with `.shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b875a53-e2dd-481e-98e2-a7983efcba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape ## shape returns the rows and colunns..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a20ad34-a948-481f-8f37-663416daeda7",
   "metadata": {},
   "source": [
    "So, our dataset has 150 rows and 5 columns. Now let's see how many different types of iris are in it. We'll do this by counting the unique values in the column called `class` with `.nunique()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5913c8b-3a8a-4082-8c9d-58eea18607e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To determine the number of iris species in the dataset,\n",
    "## we'll count the number of unique values in the column called `class`.\n",
    "df['class'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7847fb8-c7e9-4ef3-b742-5875dc6de37e",
   "metadata": {},
   "source": [
    "And we get the number we expected, 3. So that's good! Now let's print out the names of the 3 species with `.unique()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ee5364-06d6-4960-9921-23d12c524e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can print out the unique values in a dataframe's column with the 'unique()' method.\n",
    "df['class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ef960e-1a0a-4eac-929b-22489228b362",
   "metadata": {},
   "source": [
    "So, just as we expected, we see that we have 3 different species of iris in our dataset: **Setosa**, **Versicolor**, **Virginica**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1401a7fa-1dc8-402f-b4a1-cec544eb0f3e",
   "metadata": {},
   "source": [
    "Now let's verify that our dataset is balanced, meaning we have roughly the same number of entries (rows) in our data for each of the 3 iris species that we want our neural network to classify. We can do this with a fancy `for` loop that prints out the number of rows per class, regardless of the number of classes we have in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c6daa-bc21-4d53-a91f-c48fa4844d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name in df['class'].unique(): # for each unique class name...\n",
    "    \n",
    "    ## ...print out the number of rows associated with it\n",
    "    print(class_name, \": \", sum(df['class'] == class_name), sep=\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc6a6e8-0398-4507-be5a-4aa9a76b4d90",
   "metadata": {},
   "source": [
    "In this case, our dataset isn't just relatively well balanced, it is exactly balanced, and each class has exactly 50 rows of data associated with it. However, if things were really skewed, for example, we had 100 rows of data for **Setosa**, 100 rows for **Versicolor**, and only 10 rows of data for **Virginica**, then we might need to find some way to make the data more balanced. Balancing datasets is way out of the scope of this tutorial, but if you'd like to learn more with this simple **[Google search](https://www.google.com/search?q=how+to+balance+datasets)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4018d2c-7f27-40a1-ad71-b75b33f1ddbd",
   "metadata": {},
   "source": [
    "Now, let's split the data into **training** and **testing** datasets. The first step is to separate the columns into input values and labels.\n",
    "\n",
    "In this example, to keep the neural network simple, we'll just use `petal_width` and `sepal_width` values for the inputs. So the first we'll do is make sure we can correctly isolate the columns we want from the columns we don't want. We do this by passing `df` a list of column names we want to get values for, `['petal_width', 'sepal_width']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcdc455-78a9-4db7-88ed-cddf769b1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print out the first few rows of just the `petal_width` and `sepal_width` columns\n",
    "df[['petal_width', 'sepal_width']].head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a37a2f-20a8-4f04-baa3-ae13cd9566dd",
   "metadata": {},
   "source": [
    "Now that we have confirmed that we can correctly isolate the values for `petal_width` and `sepal_width`, let's use the original DataFrame, `df`, to create two new DataFrames. One DataFrame will have the petal and sepal widths, the values we will use to make predictions, and we'll call this DataFrame `input_values`. The other DataFrame will have the species, the values we will use to determine how good those predictions are, and this DataFrame will be called `label_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae66da-a69f-4ac2-8f64-cad40b5c4daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = df[['petal_width', 'sepal_width']]\n",
    "input_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cee969-3620-4c84-b6e4-6a115229587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_values = df['class']\n",
    "label_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1713310a-7341-49a1-bb7a-a1f5c989a459",
   "metadata": {},
   "source": [
    "Now, because neural networks expect the inputs and output values to be numbers, we need to convert the values in the `label_values` into numbers, and we'll do this with `factorize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c8f6af-e49a-4248-a024-020a77225677",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the strings in the 'class' column into numbers with factorize()\n",
    "classes_as_numbers = label_values.factorize()[0] ## NOTE: factorize() returns a list of lists, \n",
    "                                                 ## and since we only need the first list of values,\n",
    "                                                 ## we index the output of factorize() with [0].\n",
    "classes_as_numbers ## print out the numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2084e9-98d1-4041-93ec-8529a3ee9ce0",
   "metadata": {},
   "source": [
    "As we can see, the strings were converted into numbers. The first 50 values are 0, which represents **Setosa**. The following 50 values are 1, for **Versicolor**, and the last 50 values are 2, for **Viriginica**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f666186a-93e1-42ab-b2b8-cad73602dd58",
   "metadata": {},
   "source": [
    "Now, we need to split `input_values` and `classes_as_numbers` into **training** and **testing datasets**. And we do this with the **[sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)** function `train_test_split()`. **NOTE:** In practice, people usually use anywhere from 25-33% of the data for testing how well the model was trained. In this case, we'll use 25%, which is the default, but any percentage can be specified by setting the `test_size` parameter to a value between 0 and 1. Also, because we want to ensure that our test dataset has data for all three species of iris, we'll set `stratify=label_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de576eb-43fc-425a-8164-5f1f37d1dfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train, input_test, label_train, label_test = train_test_split(input_values,\n",
    "                                                                    classes_as_numbers, \n",
    "                                                                    test_size=0.25,\n",
    "                                                                    stratify=classes_as_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab8389e-1c0e-4c16-9990-745a7535b9cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we can verify that `train_test_split()` correctly put 75% of the data into `input_train` and `input_test` by printing out their shapes. Remember 75% of 150 = 112.5, so we would expect both `input_train` and `input_test` to have 112 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bdf25a-ebaa-4f20-99f3-21934064f532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd59c3a6-e5c1-4542-952c-c9492e0bb724",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e518f978-bf7a-4842-a8d2-d473407992ce",
   "metadata": {},
   "source": [
    "Hooray!!! Both `input_train` and `label_train` have 112 rows, which is what we expect. Now, let's verify that the remaining 38 rows of data went into `input_test` and `label_test` by printing out their shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd4630-49e7-4c3d-8111-ea3d89fa124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e4a53-0d44-46bb-8bd2-ac3329d4cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc316911-298b-40e9-b179-bad8a25cca98",
   "metadata": {},
   "source": [
    "Hooray again! `train_test_split()` did what we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e069b36-9403-4bf7-a374-24151154f6ab",
   "metadata": {},
   "source": [
    "Now, because our neural network will have 3 outputs, one for each species (see the drawing of the neural network above), we need to convert the numbers in `label_train` into 3 element arrays, where each element in an array corresponds to a specific output in the neural network. Specifically, we'll use `[1.0, 0.0, 0.0]` to correspond to **Setosa**, `[0.0, 1.0, 0.0]` for  **Versicolor**, and `[0.0, 0.0, 1.0]` for **Virginica**. The good news is that we can easily do the **[one-hot encoding](https://youtu.be/589nCGeWG1w)**. We also tack on `type(torch.float32)` to ensure the numbers are saved in the correct format for the neural network to process efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49342d42-6b32-42a5-85a4-275339c6ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now create a new tensor with one-hot encoded rows for each row in the original dataset.\n",
    "one_hot_label_train = F.one_hot(torch.tensor(label_train)).type(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c5915-3b27-4c70-9c63-867d474be9fd",
   "metadata": {},
   "source": [
    "**NOTE**: If we printed out the entire contents of `one_hot_label_train`, we'd get a matrix with 150 rows, which would take up a lot of space. So, instead, let's print out the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31ea02e-566b-4792-af27-cd6f0317402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print out a few of the rows one-hot encoded data.\n",
    "one_hot_label_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40525279-c1d3-4940-b505-f742230cd570",
   "metadata": {},
   "source": [
    "So, as we can see in the output above, `classes_as_numbers` was correctly one-hot encoded and saved in `one_hot_label_train`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac937a1-5caa-4d76-9685-8a6d811c27b3",
   "metadata": {},
   "source": [
    "Now, let's normalize the input variables so that their values range from 0 to 1. Normalizing data, so that it's all on the same scale, often makes it easier to train machine learning methods. In this case, since we have two datasets, `input_train` and `input_test`, we'll start determining the maximum and minimum values in `input_train`. Then we will use those values to normalize `input_train` and `input_test`. Using the maximum and minimum values from `input_train` to normalize both datasets avoids something called **Data Leakage**.\n",
    "\n",
    "**NOTE:** If you don't know what it means to **normalize** your data, check out this **[short song](https://youtube.com/shorts/oZ9SrkF_-LE?feature=share)** that has a good beat, and you can dance to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5185697-1816-44c1-80b5-2e7e2da7b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, determine the maximum values in input_train...\n",
    "max_vals_in_input_train = input_train.max()\n",
    "## Now print them out...\n",
    "max_vals_in_input_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5355db-7f14-48ae-a93e-7d66c2f4daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Second, determine the minimum values in input_train\n",
    "min_vals_in_input_train = input_train.min()\n",
    "## Now print them out...\n",
    "min_vals_in_input_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8628abe-9ebe-4ccf-80cd-10e697d94b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now normalize input_train with the maximum and minimum values from input_train\n",
    "input_train = (input_train - min_vals_in_input_train) / (max_vals_in_input_train - min_vals_in_input_train)\n",
    "input_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c49642-aef9-402b-9473-6bc07a712fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now normalize input_test with the maximum and minimum values from input_train\n",
    "input_test = (input_test - min_vals_in_input_train) / (max_vals_in_input_train - min_vals_in_input_train)\n",
    "input_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383057be-98c7-4741-b3cd-20aa54a94d9a",
   "metadata": {},
   "source": [
    "Now, let's put our training data into a **DataLoader**, which we can use to train the neural network. **DataLoaders** are great for large datasets because they make it easy to access the data in batches, make it easy to shuffle the data each epoch, and they make it easy to use a relatively small fraction of the data if we want to do a quick and dirty training for debugging our code.\n",
    "\n",
    "To put our data training data into a **DataLoader**, we'll start by converting `input_train` into tensors with `torch.tensor()`. We'll then combine `'input_train` with `one_hot_label_train` to create a **TensorDataset**. Lastly, we'll use the **TensorDataset** to create the **DataLoader**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fedb584-dcb0-4e1b-ba22-f7ebaa40cfb0",
   "metadata": {},
   "source": [
    "**NOTE:** `torch.tensor()` will get all bent out of shape if we pass it a DataFrame directly. So, instead of passing it a DataFrame, we pass it the values by tacking `.values` on to the end of each DataFrame. We also tack on `type(torch.float32)` to make sure the numbers are saved in the correct format for the neural network to process efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96216cf7-498e-4498-beb7-9e1b50a7bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the DataFrame input_train into tensors\n",
    "input_train_tensors = torch.tensor(input_train.values).type(torch.float32)\n",
    "\n",
    "## now print out the first 5 rows to make sure they are what we expect.\n",
    "input_train_tensors[:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de6d9df-347e-4017-bed7-80244e8c52fa",
   "metadata": {},
   "source": [
    "**NOTE:** Because we'll also need to run `input_test` through the neural network, we'll need to convert it to tensors as well, and we might as well do it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d15911-d4b0-4ccc-88ea-055a2be920a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the DataFrame input_test into tensors\n",
    "input_test_tensors = torch.tensor(input_test.values).type(torch.float32)\n",
    "\n",
    "## now print out the first 5 rows to make sure they are what we expect.\n",
    "input_test_tensors[:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8dd94d-c620-415c-8d06-3267e415c1d9",
   "metadata": {},
   "source": [
    "Now that we have tensors for `input_train`, named `input_train_tensors`, and we have the one-hot encoded `class` values stored in tensors called `label_train`, we can combine them into a **TensorDataset** that are, in turn, turned into **DataLoader**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a23b9c-ae55-41cd-9828-db0f0eb39e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(input_train_tensors, one_hot_label_train) \n",
    "train_dataloader = DataLoader(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31955e37-156e-4c5b-b4bd-806d0e6e2d74",
   "metadata": {},
   "source": [
    "# BAM!\n",
    "\n",
    "At long last, we have created the **DataLoaders** that we need to train and test a neural network. Now, let's build the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3fb953-440f-4930-a018-050070e67040",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b97945-a5bb-47d2-a07a-04b0dce01539",
   "metadata": {},
   "source": [
    "<a id=\"build\"></a>\n",
    "# Building a neural network with multiple inputs and outputs with PyTorch and Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ed8a32-4aeb-4a4e-9c6c-4bfff11ba0ea",
   "metadata": {},
   "source": [
    "Building a neural network with PyTorch means creating a new class. And to make it easy to train the neural network, this class will inherit from `LightningModule`.\n",
    "\n",
    "Our new class will have the following methods:\n",
    "- `__init__()` to initialize the Weights and Biases and keep track of a few other housekeeping things.\n",
    "- `forward()` to make a forward pass through the neural network.\n",
    "- `configure_optimizers()` to configure the optimizer. There are lots of optimizers to choose from, but in this tutorial, we'll change things up and use `Adam`.\n",
    "- `training_step()` to pass the training data to `forward()`, calculate the loss and keep track of the loss values in a log file.\n",
    "\n",
    "Also, for reference, here is a picture of the neural network we want to create:\n",
    "<img src=\"./images/chapter_4_softmax.png\" alt=\"a neural network with multiple inputs and outputs and softmax\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f26851-96df-4a84-b1ce-3351e61adf25",
   "metadata": {},
   "source": [
    "As we can see in the picture, our neural network has 2 inputs, one for Petal Width and one for Sepal Width, a single hidden layer with two **[ReLU](https://youtu.be/68BZ5f7P94E)** activation functions, and 3 outputs, one for each species of iris.\n",
    "\n",
    "So, given this specification for this neural network, let's code it in a new class called `MultipleInsOuts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611711c1-ea20-48fa-bb89-c2ce5074dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleInsOuts(L.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__() ## We call the __init__() for the parent, LightningModule, so that it\n",
    "                           ## can initialize itself as well.\n",
    "        \n",
    "        ## Now we the seed for the random number generorator.\n",
    "        ## This ensures that when you create a model from this class, that model\n",
    "        ## will start off with the exact same random numbers that I started out with when\n",
    "        ## I created this demo. At least, I hope that is what happens!!! :)\n",
    "        L.seed_everything(seed=42)\n",
    "        \n",
    "        ############################################################################\n",
    "        ##\n",
    "        ## Here is where we initialize the Weights and Biases for the neural network\n",
    "        ##\n",
    "        ############################################################################\n",
    "        \n",
    "        ## If you look at the drawing of the network we want to build (above), \n",
    "        ## you see that we have 2 inputs that lead to 2 activation functions.\n",
    "        ## We create these connections and initialize their Weights and Biases\n",
    "        ## with the nn.Linear() function by setting in_features=2 and out_features=2.\n",
    "        self.input_to_hidden = nn.Linear(in_features=2, out_features=2, bias=True)\n",
    "        \n",
    "        ## Next, we see that the 2 activation functions are connected to 3 outputs.\n",
    "        ## We create these connections and initialize their Weights and Biases\n",
    "        ## with the nn.Linear() function by setting in_features=2 and out_features=3. \n",
    "        self.hidden_to_output = nn.Linear(in_features=2, out_features=3, bias=True)\n",
    "        \n",
    "        ## We'll use Cross Entropy to calculate the loss between what the \n",
    "        ## neural network's predictions and actual, or known, species for\n",
    "        ## each row in the dataset.\n",
    "        ## To learn more about Cross Entropy, see: https://youtu.be/6ArSys5qHAU\n",
    "        ## NOTE: nn.CrossEntropyLoss applies a SoftMax function to the values\n",
    "        ## we give it, so we don't have to do that oursevles. However,\n",
    "        ## when we use this neural network (after it has been trained), we'll\n",
    "        ## have to remember to apply a SoftMax function to the output.\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    def forward(self, input): \n",
    "        ## First, we run the input values to the activation functions\n",
    "        ## in the hidden layer\n",
    "        hidden = self.input_to_hidden(input)\n",
    "        ## Then we run the values through a ReLU activation function\n",
    "        ## and then run those values to the output.\n",
    "        output_values = self.hidden_to_output(torch.relu(hidden))\n",
    "            \n",
    "        return(output_values)\n",
    "        \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        ## In this example, configuring the optimizer\n",
    "        ## consists of passing it the weights and biases we want\n",
    "        ## to optimize, which are all in self.parameters(),\n",
    "        ## and setting the learning rate with lr=0.001.\n",
    "        return Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        ## The first thing we do is split 'batch'\n",
    "        ## into the input and label values.\n",
    "        inputs, labels = batch \n",
    "        \n",
    "        ## Then we run the input through the neural network\n",
    "        outputs = self.forward(inputs)\n",
    "        \n",
    "        ## Then we calculate the loss.\n",
    "        loss = self.loss(outputs, labels)\n",
    "        \n",
    "        ## Lastly, we could add the loss a log file\n",
    "        ## so that we can graph it later. This would\n",
    "        ## help us decide if we have done enough training\n",
    "        ## Ideally, if we do enough training, the loss\n",
    "        ## should be small and not getting any smaller.\n",
    "        # self.log(\"loss\", loss) \n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a1ec20-c2d3-45f7-8ce7-e1c1df469f6d",
   "metadata": {},
   "source": [
    "Now that we've created a class for our neural network, let's train it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921a6dd7-44ec-44ba-bffa-e0ab6eaa59d9",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645699fb-e215-46f2-b730-2e9276d0a407",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"train\"></a>\n",
    "# Training our Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a3cac-ccb9-49d2-96fc-b797d7e0823c",
   "metadata": {},
   "source": [
    "Training our new neural network means we create a model from the new class, `MultipleInsOuts`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34ca1ef-4635-4630-a4d0-0ddf5bb2022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultipleInsOuts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb353a83-3690-4a36-a164-e60e499bc79f",
   "metadata": {},
   "source": [
    "...and then create a **Lightning Trainer**, `L.Trainer`, and use it to optimize the parameters. **NOTE:** We will start with 10 epochs, complete runs through our training data. This may be enough to successfully optimize all of the parameters, but it might not. We'll find out later in the tutorial when we make a graph of how the loss values change during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5f1bc-fcef-4cac-b33e-1225ec246f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(max_epochs=10)\n",
    "trainer.fit(model, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc0b74a-127c-49aa-8168-91dd28483b5f",
   "metadata": {},
   "source": [
    "Hooray! We've trained the model with 10 epochs! Now, let's see if the predictions are any good. We can do this by seeing how well it predicts the testing data. We'll start by running `input_test_tensors` through the neural network and saving the output `predictions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcdcfd9-36f1-44d2-ae8c-3b3f78e99693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the input_test_tensors through the neural network\n",
    "predictions = model(input_test_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d258a469-33ae-4941-a5b8-86f9b1499ac4",
   "metadata": {},
   "source": [
    "Now, because our neural network has three outputs, one for **Setosa**, one for **Versicolor**, and one for **Virginica**, we should get 3 values for each row in `input_test_tensors`. We can verify that by looking at the first few rows of `predictions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec2ed77-e863-40fb-9142-2da3ec9f3f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0:4,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52512e0f-9a49-4080-aeea-ceac0745ca5a",
   "metadata": {},
   "source": [
    "We can determine which species was predicted in `predictions` by selecting the index in each row that corresponding to the largest value, and we do that with `torch.argmax()`. `torch.argmax()` returns a tensor that contains the indices with the largest values for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a336e70-1e01-48a2-9761-cb51c845dbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select the output with highest value...\n",
    "predicted_labels = torch.argmax(predictions, dim=1) ## dim=0 applies argmax to rows, dim=1 applies argmax to columns\n",
    "predicted_labels[0:4] # print out the first 4 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb622159-b415-4e2e-83af-328cabad438e",
   "metadata": {},
   "source": [
    "In the first row index 0 had the largest value. Thus, the first prediction corresponds to **Setosa**. The second, third, and fourth rows predicted 2, which corresponds to **Virginica**.\n",
    "\n",
    "Now, let's compare what the neural network predicted in `predicted_labels` to the known values in `label_test` and calculate the percentage of correct predictions. We do this by adding up the number of times an element in `predicted_labels` equals the corresponding element in `label_test` and dividing by the number of elements in `predicted_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0872e381-57ef-43e3-a6b4-9aa9d96c93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now compare predicted_labels with test_labels to calculate accuracy\n",
    "## NOTE: torch.eq() computes element-wise equality between two tensors.\n",
    "##       label_test, however, is just an array, so we convert it to a tensor\n",
    "##       before passing it in. torch.sum() then adds up all of the \"True\"\n",
    "##       output values to get the number of correct predictions. \n",
    "##       We then divide the number of correct predictions by the number of predicted values,\n",
    "##       obtained with len(predicted_labels), to get the percentage of correct predictions\n",
    "torch.sum(torch.eq(torch.tensor(label_test), predicted_labels)) / len(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd75320-d89e-4928-84ba-9b3baaa966c5",
   "metadata": {},
   "source": [
    "And we see that our neural network only correctly predicts 66% of the testing data. This isn't very good. So, will training our model for more epochs improve the model's predictions?\n",
    "\n",
    "One way to answer that question is to just train for longer and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261fdd41-2caa-43be-8c78-0c2d64f08f3d",
   "metadata": {},
   "source": [
    "The good news is that because we're using **Lightning**, we can pick up where we left off training without starting over from scratch. This is because training with **Lightning** creates _checkpoint_ files that keep track of the Weights and Biases as they change. As a result, all we have to do to pick up where we left off is tell the `Trainer` where the checkpoint files are. This is awesome and will save us a lot of time since we don't have to retrain the first **10** epochs. So, let's add an additional **90** epochs to the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b8180-976d-4c1e-b2e7-cb45cba434c9",
   "metadata": {},
   "source": [
    "To add additional epochs to the training, we first identify where the checkpoint file is with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc4c1b-911d-417f-8ab2-9f52712502e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_checkpoint = trainer.checkpoint_callback.best_model_path ## By default, \"best\" = \"most recent\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4fd497-997c-4369-9b3e-d25cd0911d5d",
   "metadata": {},
   "source": [
    "Then we create a new Lightning Trainer, just like before, but we set the number of epochs to 100. Given that we already trained for 10 epochs, this means we'll do 90 more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a849d2f-bad1-4214-a72b-f1b04174639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, create a new Lightning Trainer\n",
    "trainer = L.Trainer(max_epochs=100) # Before, max_epochs=10, so, by setting it to 100, we're adding 90 more.\n",
    "\n",
    "## Then call trainer.fit() using the path to the most recent checkpoint files\n",
    "## so that we can pick up where we left off.\n",
    "trainer.fit(model, train_dataloaders=train_dataloader, ckpt_path=path_to_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca3ecd4-f56d-4a1e-89cc-1f7cf5992a1e",
   "metadata": {},
   "source": [
    "Now, let's run the testing data through the network and calculate the accuracy. We'll do this just like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930f26fe-eca7-46a3-9652-0e1d1b1bcfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the input_test_tensors through the neural network\n",
    "predictions = model(input_test_tensors)\n",
    "\n",
    "## Select the output with highest value...\n",
    "predicted_labels = torch.argmax(predictions, dim=1) ## dim=0 applies softmax to rows, dim=1 applies softmax to columns\n",
    "\n",
    "## Now compare predicted_labels with test_labels to calculate accuracy\n",
    "## NOTE: torch.eq() computes element-wise equality between two tensors.\n",
    "##       label_test, however, is just an array, so we convert it to a tensor\n",
    "##       before passing it in. torch.sum() then adds up all of the \"True\"\n",
    "##       output values to get the number of correct predictions. \n",
    "##       We then divide the number of correct predictions by the number of predicted values,\n",
    "##       obtained with len(predicted_labels), to get the percentage of correct predictions\n",
    "torch.sum(torch.eq(torch.tensor(label_test), predicted_labels)) / len(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31918cf1-5cfc-475b-b61a-f08525111741",
   "metadata": {},
   "source": [
    "After 100 training epochs, we correctly classified 95% of the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a78589d-3d39-40a4-81d1-c22caddc6879",
   "metadata": {},
   "source": [
    "# Double BAM!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79958e71-548a-49a6-a6c5-d2a5ddd279c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print out the name and value for each named parameter\n",
    "## parameter in the model. Remember parameters are variables,\n",
    "## like Weights and Biases, that we can train.\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, torch.round(param.data, decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8da8b1-4b5e-42a8-bdaa-3d939716fad2",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b568a1ad-f6d4-436f-8c58-b39371680175",
   "metadata": {},
   "source": [
    "<a id=\"predict\"></a>\n",
    "# Make a Prediction with New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2df2a0-77e3-40dd-8cf5-3a23d66e5098",
   "metadata": {},
   "source": [
    "Now that our model is trained, we can use it to make predictions from new data. This is done by passing the model a tensor with normalized petal and sepal widths wrapped up in a tensor, and then passing the model output to `torch.argmax()`.\n",
    "\n",
    "For example, if the raw petal and sepal width measurements were 0.2 and 3.0, we would first normalize them using the maximum and minimum values we calculated with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8759e954-3d0b-4ac3-84ac-5c294585643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_values = ([0.2, 3.0] - min_vals_in_input_train) / (max_vals_in_input_train - min_vals_in_input_train)\n",
    "normalized_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211e1e78-d0dd-46c8-9cd5-f59815385dea",
   "metadata": {},
   "source": [
    "Then we convert `normalized_values` into a tensor and pass it to the model, and pass the output to the `argmax()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192cb03b-44f5-4fcf-961d-426a79d40c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(model(torch.tensor(normalized_values).type(torch.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2053241f-37bb-44b3-b2da-38e841679422",
   "metadata": {},
   "source": [
    "And the output is 0, meaning that the neural network predicts that the measurements come from **Setosa**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2940946-b17a-4e9b-ac7f-02cd4f97db91",
   "metadata": {},
   "source": [
    "# TRIPLE BAM!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
