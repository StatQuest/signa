{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f40c232-df6e-49df-9016-6459e4af2e1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# StatQuest: Coding Transformers from Scratch!!!\n",
    "## Part 1: Encoder-Decoder Transformers\n",
    "\n",
    "Copyright 2024, Joshua Starmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4226d63-8d76-40bc-a8e6-0f290a159418",
   "metadata": {},
   "source": [
    "---- \n",
    "\n",
    "In this tutorial, we will use **[PyTorch](https://pytorch.org/) + [Lightning](https://www.lightning.ai/)** to create and optimize an encoder-decoder transformer, like the one shown in the picture below.\n",
    "\n",
    "<img src=\"./images/enc_dec_transformer.png\" alt=\"an encoder-decoder neural network\" style=\"width: 800px;\">\n",
    "\n",
    "In this tutorial, you will...\n",
    "\n",
    "- **[Code a Position Encoder Class From Scratch!!!](#position)** The position encoder class will give the encoder and the decoder a way to keep track of the order of the input tokens in the encoder and the decoder.\n",
    "\n",
    "- **[Code an Attention Class From Scratch!!!](#attention)** The attention class will allow us to keep track of how words in the input and output are related to each other\n",
    "\n",
    "- **[Code an Encoder Class From Scratch!!!](#encoder)** The encoder will process the input.\n",
    "\n",
    "- **[Code a Decoder Class From Scratch!!!](#decoder)** The decoder will generate the output.\n",
    "\n",
    "- **[Code a Transformer Class From Scratch!!!](#transformer)** The transformer class will connect all the pieces, the position encoder, attention, the encoder and the decoder.\n",
    "\n",
    "- **[Train the Transformer!!!](#train)** We'll train the transformer to translate simple English phrases into Spanish.\n",
    "\n",
    "- **[Use the Trained Transformer!!!](#use)** Finally we'll use the transformer to translate simple English phrases into Spanish.\n",
    "\n",
    "#### NOTE:\n",
    "This tutorial assumes that you already know the basics of coding in **Python** and are familiar with the theory behind **[Encoder-Decoder Transformers CORRECT THIS LINK]()** and **[Backpropagation](https://youtu.be/IN2XmBhILt4)**. If not, check out the **StatQuests** by clicking on the links for each topic.\n",
    "\n",
    "#### ALSO NOTE:\n",
    "I strongly encourage you to play around with the code. Playing with the code is the best way to learn from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855d3a52-a43e-44cf-b8b6-b2ce88bea382",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b86036-1369-441c-a14d-3ba7bdaa103b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import the modules that will do all the work\n",
    "\n",
    "The very first thing we need to do is load a bunch of Python modules. Python itself is just a basic programming language. These modules give us extra functionality to create and train a Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c520e0b-c6e4-43ce-93f5-c0f2b5e75438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch ## torch let's us create tensors and also provides helper functions\n",
    "import torch.nn as nn ## torch.nn gives us nn.Module, nn.Embedding() and nn.Linear()\n",
    "import torch.nn.functional as F # This gives us the softmax() and argmax()\n",
    "from torch.optim import Adam # This is the optimizer we will use\n",
    "\n",
    "import lightning as L # Lightning makes it easier to write, optimize and scale our code\n",
    "from torch.utils.data import TensorDataset, DataLoader # We'll store our data in DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e58ebb2-1798-41c3-9ff8-1b4f50605964",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9ddca3-53b6-451b-8fe1-6116e1f7f473",
   "metadata": {},
   "source": [
    "# The input and output vocabularies and data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8460ac8-e0fc-488a-9478-e7da0cc4df74",
   "metadata": {},
   "source": [
    "In this tutorial we will build a simple encoder-decoder transformer that can translate simple Engilish phrases into Spanish. Specifically, we'll be able to translate **Let's go** to **vamos** and **to go** to **ir**. In order to keep track of things,\n",
    "we'll create dictionaries for the input and output vocabularies, and then create a **Dataloader** that contains the English phrases mapped to their Spanish translations. Ultimately we'll use the **Dataloader** to train the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63631347-8db6-4812-8198-9169b2df0b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first, a dictionary for the input vocabulary\n",
    "input_vocab = {'<SOS>': 0, ## <SOS> = start of sequence.\n",
    "               'lets': 1,\n",
    "               'to': 2,\n",
    "               'go': 3}\n",
    "\n",
    "## Now a dictionary for the output vocabulary\n",
    "output_vocab = {'<SOS>': 0,\n",
    "                'ir': 1,\n",
    "                'vamos': 2,\n",
    "                'y': 3,\n",
    "                '<EOS>': 4}\n",
    "\n",
    "## Here are the english phrases, encoded using the\n",
    "## input vocabulary\n",
    "## NOTE: our transformer will prepend the <SOS> token to these inputs\n",
    "inputs = torch.tensor([[1, 3],\n",
    "                       [2, 3]])\n",
    "\n",
    "## Here are the spanish translations encoded using\n",
    "## the output vocabulary.\n",
    "## NOTE: our transformer will prepend the <SOS> token to these outputs\n",
    "labels = torch.tensor([[2],\n",
    "                      [1]])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels) \n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de72068-c081-4868-9e60-1f5cc6cf45df",
   "metadata": {},
   "source": [
    "Now that we have created the input and output datasets and the **Dataloader** to train the model, let's start building it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb1335c-7bd0-4c60-80c1-6043ee11accd",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42efbdf7-b3de-4b12-b2ec-a54651b9df79",
   "metadata": {},
   "source": [
    "<a id=\"position\"></a>\n",
    "# Position Encoding\n",
    "\n",
    "Position Encoding helps the transformer keep track of the order of the words in the input and the output. For example, in the picture below, we see that the two phrases **Squatch eats pizza** and **Pizza eats Squatch** both have the exact same words, but, due to differences in the word order, have very different meanings. Thus, keeping track of word order is very important.\n",
    "\n",
    "<img src=\"./images/squatch_eats_pizza.png\" alt=\"Squatch eats pizza is very different from Pizza eats Squatch\" style=\"width: 800px;\">\n",
    "\n",
    "There are a bunch of ways for a transformer to keep track of word order, but one popular method is to use a series of alternating sine and cosine curves (seen below). The number of sine and cosine squiggles depends on how many numbers, or word embedding values, we use to represent each token. In the context of Transformers, the number of numbers, or word embedding values, we use to to represent each token is the **dimension** of the transformer. So, if the transformer's dimension is 2, meaning that it uses 2 numbers to represent each token, then we only need one sine and one cosine squiggle. \n",
    "\n",
    "<img src=\"./images/pos_encoding_1.png\" alt=\"Sine and cosine squiggles for position encoding\" style=\"width: 800px;\">\n",
    "\n",
    "In contrast, as we see in the illustration below, if the transformer's dimension is 4, then we'll need 2 sine squiggles alternating with 2 cosine squiggles, for a total of 4 squiggles.\n",
    "\n",
    "<img src=\"./images/pos_encoding_2.png\" alt=\"More sine and cosine squiggles for position encoding\" style=\"width: 800px;\">\n",
    "\n",
    "As we see in the illustration above, the additional pair of sine and cosine squiggles have a wider period (they repeat less frequently) than the first pair. Increasing the period for each additional pair of squiggles ensures that each position is represented by a unique combination of values.\n",
    "\n",
    "**NOTE:** The reason why we are bothering to create a class to do positional encoding, instead of just adding this code directly to the transformer, is that both the Encoder and Decoder need to use it. So, by creating a class that does positional encoding, we can code it once, and then just create as many instances of it as we need (which, in this case, is two times).\n",
    "\n",
    "**ALSO NOTE:** Since the position encoding values never change, meaning that the first token always uses the same position encoding values regardless of what that token is, we can precompute them and save them in a lookup table. This makes adding position encoding values super fast.\n",
    "\n",
    "Now that we understand the ideas that we want to implement in the Position Encoding class, let's code it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b62789-ee84-49bf-b736-12c1b47b34ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "    def __init__(self, d_model=2, max_len=3):\n",
    "        ## d_model = The dimension of the transformer, which is also the number of embedding values per token.\n",
    "        ##           In the transformer I used in the StatQuest: Transformer Neural Networks Clearly Explained!!!\n",
    "        ##           d_model=2, so that's what we'll use as a default for now.\n",
    "        ##           However, in \"Attention Is All You Need\" d_model=512\n",
    "        ## max_len = maximum number of tokens we allow as input.\n",
    "        ##           Since we are precomputing the position encoding values and storing them in a lookup table\n",
    "        ##           we can use d_model and max_len to determine the number of rows and columns in that\n",
    "        ##           lookup table.\n",
    "        ##\n",
    "        ##           In this simple example, we are only using 2 word phrases + <SOS>, so we are using\n",
    "        ##           max_len=3 as the default setting.\n",
    "        ##           However, in The Annotated Transformer, they set the default value for max_len to 5000\n",
    "        \n",
    "        super().__init__()\n",
    "        ## We call the super's init because by creating our own init method, we overwrite the one\n",
    "        ## we inherited from nn.Module. So we have to explicity call nn.Module's __init__(), otherwise it\n",
    "        ## won't get initialized. NOTE: If we didn't write our own __init__(), then we would not have\n",
    "        ## to call super().__init__(). Alternatively, if we didn't want to access any of nn.Module's methods, \n",
    "        ## we wouldn't have to call it then either.\n",
    "\n",
    "        ## Now we create a lookup table, pe, of position encoding values and initialize all of them to 0.\n",
    "        ## To do this, we will make a matrix of 0s that has max_len rows and d_model columns.\n",
    "        ## for example...\n",
    "        ## torch.zeros(2, 3)\n",
    "        ## ...returns a matrix of 0s with 2 rows and 3 columns...\n",
    "        ## tensor([[0., 0., 0.],\n",
    "        ##         [0., 0., 0.]])\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        ## Now we create a sequence of numbers for each position that a token can have in the input (or output).\n",
    "        ## For example, if the input tokens where \"I'm happy today!\", then \"I'm\" would get the first\n",
    "        ## position, 0, \"happy\" would get the second position, 1, and \"today!\" would get the third position, 2.\n",
    "        ## NOTE: Since we are going to be doing math with these position indices to create the \n",
    "        ## positional encoding for each one, we need them to be floats rather than ints.\n",
    "        ## \n",
    "        ## NOTE: Two ways to create floats are...\n",
    "        ##\n",
    "        ## torch.arange(start=0, end=3, step=1, dtype=torch.float)\n",
    "        ##\n",
    "        ## ...and...\n",
    "        ##\n",
    "        ## torch.arange(start=0, end=3, step=1).float()\n",
    "        ##\n",
    "        ## ...but the latter is just as clear and requires less typing.\n",
    "        ##\n",
    "        ## Lastly, .unsqueeze(1) converts the single list of numbers that torch.arange creates into a matrix with\n",
    "        ## one row for each index, and all of the indices in a single column. So if \"max_len\" = 3, then we\n",
    "        ## would create a matrix with 3 rows and 1 column like this...\n",
    "        ##\n",
    "        ## torch.arange(start=0, end=3, step=1, dtype=torch.float).unsqueeze(1)\n",
    "        ##\n",
    "        ## ...returns...\n",
    "        ##\n",
    "        ## tensor([[0.],\n",
    "        ##         [1.],\n",
    "        ##         [2.]])        \n",
    "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "\n",
    "\n",
    "        ## Here is where we start doing the math to determine the y-axis coordinates on the\n",
    "        ## sine and cosine curves.\n",
    "        ##\n",
    "        ## The positional encoding equations used in \"Attention is all you need\" are...\n",
    "        ##\n",
    "        ## PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "        ## PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "        ##\n",
    "        ## ...and we see, within the sin() and cos() functions, we divide \"pos\" by some number that depends\n",
    "        ## on the index and number of PE values we want per token (d_model). So, pretty much everyone\n",
    "        ## calculates the term we use to divide \"pos\" by first, and they do it with code that looks like this...\n",
    "        ##\n",
    "        # div_term = torch.exp(torch.arange(start=0, end=d_model, step=2).float() * -(math.log(10000.0) / d_model))\n",
    "        ##\n",
    "        ## NOTE: The fact that div_term = 1/(10000^(2i/d_model)) is not immediately clear for a few reasons: \n",
    "        ##\n",
    "        ##    1) div_term wrapps everything in a call to torch.exp() \n",
    "        ##    2) It uses log()\n",
    "        ##    2) The order of the terms is different \n",
    "        ##\n",
    "        ## The reason for these differences is, presumably, trying to prevent underflow (getting too close to 0).\n",
    "        ## So, to show that div_term = 1/(10000^(2i/d_model))...\n",
    "        ##\n",
    "        ## 1) Swap out math.log() for torch.log() (doing this requires converting 10000.0 to a tensor, which is my\n",
    "        ##    guess for why they used math.log() instead of torch.log())...\n",
    "        ## torch.exp(torch.arange(start=0, end=d_model, step=2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        ##\n",
    "        ## 2) Rearrange the terms...\n",
    "        ## torch.exp(-1 * (torch.log(torch.tensor(10000.0)) * torch.arange(start=0, end=d_model, step=2).float() / d_model))\n",
    "        ##\n",
    "        ## 3) Pull out the -1 by with exp(-1 * x) = 1/exp(x)\n",
    "        ## 1/torch.exp(torch.log(torch.tensor(10000.0)) * torch.arange(start=0, end=d_model, step=2).float() / d_model)\n",
    "        ##\n",
    "        ## 4) Use exp(a * b) = exp(a)^b to pull out the 2i/d_model term...\n",
    "        ## 1/torch.exp(torch.log(torch.tensor(10000.0)))^(torch.arange(start=0, end=d_model, step=2).float() / d_model)\n",
    "        ##\n",
    "        ## 5) Use exp(log(x)) = x to get the original form of the denominator...\n",
    "        ## 1/torch.tensor(10000.0)^(torch.arange(start=0, end=d_model, step=2).float() / d_model)\n",
    "        ##\n",
    "        ## 6) Bam.\n",
    "        ## \n",
    "        ## So, that being said, I don't think underflow is actually that big an issue. In fact, some coder at Hugging Face\n",
    "        ## also doesn't think so, and their code for positional encoding in DistilBERT (a streamlined version of BERT, which\n",
    "        ## is a transformer model)\n",
    "        ## calculates the values directly - using the form of the equation found in original Attention is all you need\n",
    "        ## manuscript. See...\n",
    "        ## https://github.com/huggingface/transformers/blob/455c6390938a5c737fa63e78396cedae41e4e87e/src/transformers/modeling_distilbert.py#L53\n",
    "        ## So I think we can simplify the code, but also have some notes that show how it is equivalent to what\n",
    "        ## you'll see in the wild...\n",
    "        div_term = 1/torch.tensor(10000.0)**(torch.arange(start=0, end=d_model, step=2).float() / d_model)\n",
    "        \n",
    "        ## Now we calculate the actual positional encoding values. Remember 'pe' was initialized as a matrix of 0s\n",
    "        ## with max_len (max number of input tokens) rows and d_model (number of embedding values per token) columns.\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) ## every other column, starting with the 1st, has sin() values\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) ## every other column, starting with the 2nd, has cos() values\n",
    "        ## NOTE: If the notation for indexing 'pe[]' looks cryptic to you, read on...\n",
    "        ##\n",
    "        ## First, let's look at the general indexing notation:\n",
    "        ##\n",
    "        ## i:j:k = select elements between i and j with stepsize = k.\n",
    "        ##\n",
    "        ## i defaults to 0\n",
    "        ## j defaults to the number of elements in the row, column or whatever.\n",
    "        ## k defaults to 1\n",
    "        ##\n",
    "        ## Now that we have looked at the general notation, let's look at specific\n",
    "        ## examples so that we can understand it.\n",
    "        ##\n",
    "        ## We'll start with: pe[:, 0::2]\n",
    "        ##\n",
    "        ## The stuff that comes before the comma refers to the rows we want to select.\n",
    "        ## In this case, we have ':' before the comma, and that means \"select all rows\".\n",
    "        ## This is because we are using the default values for i, j and k.\n",
    "        ##\n",
    "        ## The stuff after the comma refers to the columns we want to select.\n",
    "        ## In this case, we have '0::2', and that means we start with\n",
    "        ## the first column (column =  0) and go to the end (using the default value for j)\n",
    "        ## and we set the stepsize to 2, which means we skip every other column.\n",
    "        ##\n",
    "        ## Now to understand pe[:, 1::2]\n",
    "        ##\n",
    "        ## Again, the stuff before the comma refers to the rows, and, just like before\n",
    "        ## we use default values for i,j and k, so we select all rows.\n",
    "        ##\n",
    "        ## The stuff that comes after the comma refers to the columns.\n",
    "        ## In this case, we start with the 2nd column (column = 1), and go to the end\n",
    "        ## (using the default value for 'j') and we set the stepsize to 2, which\n",
    "        ## means we skip every other column.\n",
    "        ##\n",
    "        ## NOTE: using this ':' based notation is called \"indexing\" and also called \"slicing\"\n",
    "        \n",
    "        ## Now we \"register 'pe'.\n",
    "        self.register_buffer('pe', pe) ## using \"register_buffer()\" prevents \"pe\" from getting\n",
    "                                       ## passed to the optimizer. Thus, ultimately, these tensors will\n",
    "                                       ## not get optimized, which is what we want.\n",
    "                                       ##\n",
    "                                       ## NOTE: If, instead, we had set \"requires_grad=False\", then \n",
    "                                       ## \"pe\" would still get passed to the optimizer (when we pass it\n",
    "                                       ## model.parameters()) and the optimizer would have to skip over them\n",
    "                                       ## so doing this with \"register_buffer()\" is a little cleaner.\n",
    "\n",
    "    ## The forward() method is what is called by default when we use a PositionEncoding() object.\n",
    "    ## In other words, after we create a PositionEncoding() object, pe = PositionEncoding(), we\n",
    "    ## can add position encoding values to the word embeddings with pe(word_embeddings).\n",
    "    def forward(self, x):\n",
    "        # x = word embedding values\n",
    "        # x.size(0) returns the number of embedding values\n",
    "        return x + self.pe[:x.size(0), :] ## NOTE: That second ':' is optional and we could re-write it these ways\n",
    "                                          ## self.pe[:x.size(0)] = self.pe[:x.size(0), :]\n",
    "                                          ## The first is the least amount of typing, the last is the most explicit\n",
    "                                          ## which is what I prefer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fe202d-6995-4322-ad5c-119ef7dc4b28",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20eccdc-3188-4c6d-94da-d28db057dc24",
   "metadata": {},
   "source": [
    "<a id=\"attention\"></a>\n",
    "# Attention\n",
    "We're going to code an `Attention` class to do all of the types of attention that a transformer might need: **Self-Attention**, **Masked Self-Attention** (which is used by the Decoder during training) and **Encoder-Decoder Attention**.\n",
    "\n",
    "**Self-Attention** is a type of attention used in Encoder-Decoder transformers and Encoder-Only transformers. It allows every word in a phrase to define a relationship with any other word in the phrase, regardless of the order of the words. In other words, if the the phrase is **The pizza came out of the oven and it tasted good!**, then the word **it** can define it's relationship with every word in that phrase, including words that came after it, like **tasted** and **good**, as illustrated by the blue arrows in the figure below.\n",
    "\n",
    "<img src=\"./images/self_attention_1.png\" alt=\"An illustration of self-attention\" style=\"width: 800px;\">\n",
    "\n",
    "**Masked Self-Attention** is used by all types of transformers (Encoder-Only, Encoder-Decoder, and Decoder-Only) and it allows each word in a phrase to define a relationship with itself and the words that came before it. In other words, **Masked Self-Attention** prevents the transformer from \"looking ahead\". This is illustrated below where the word **it** can define relationships with itself and everything that came before. In Encoder-Only and Encoder-Decoder transformers, **Masked Self-Attention** is used during training, when we know what the output should be, but we still force the decoder to generate it one token at a time, and thus, limiting attention to only output words that came earlier. In contrast, Decoder-Only transformers use **Masked Self-Attention** all the time, on the input and the output, during training and during inference. Thus, even though the Decoder-Only transformer doesn't have to generate the input, and thus, can see all of it during training and during inference, it still only allows the attention values for each word to depend on words that came before it.\n",
    "\n",
    "<img src=\"./images/masked_attention_1.png\" alt=\"An illustration of Masked Self-Attention\" style=\"width: 800px;\">\n",
    "\n",
    "**Encoder-Decoder Attention** is only used in Encoder-Decoder transformers, where there is a distinct seperation of the part of the transformer that processes in the input (the encoder) from the part that generates the output (the decoder). **Encoder-Decoder Attention** lets each word in the output (in the decoder) define relationships with all the words in the input (in the encoder), as illustrated in the figure below.\n",
    "\n",
    "<img src=\"./images/enc_dec_attention_1.png\" alt=\"An illustration of Encoder-Decoder Attention\" style=\"width: 800px;\">\n",
    "\n",
    "Now that we have a general sense of three types of attention used in transformers, we can talk about how it's calculated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749d3c97-616d-4af1-a463-960fc36fdcf6",
   "metadata": {},
   "source": [
    "First, the general equations for the different types of attention are almost the identical as seen in the figure below. In the equations, **Q** is for the **Query** matrix, **K** is for the **Key** matrix and **V** is for the **Value** matrix. On the left, we have the equation for Self-Attention and Encoder-Decoder Attention. As we see the differences in these types of attention are not from the equation we use, but how the **Q**, **K**, and **V** matrices are computed. On the right, we see the equation for Masked Self-Attention and the only difference it has from the equation on the left is the addition of a **Mask** matrix, **M**, that prevents words that come after a specific **Query** from being included in the final attention scores. \n",
    "\n",
    "<img src=\"./images/attention_equations.png\" alt=\"Equations for computing attention\" style=\"width: 800px;\">\n",
    "\n",
    "**NOTE:** Since both equations are very similar, we'll go through one example and point out the key differences when we get to them.\n",
    "\n",
    "First, given word embedding values for each word/token in the input phrase **\\<SOS> let's go** in matrix form, we multiply them by matrices of weights to create **Queries**, **Keys** and **Values** \n",
    "\n",
    "<img src=\"./images/attention_compute_q.png\" alt=\"Computing the Q matrix\" style=\"width: 800px;\">\n",
    "\n",
    "<img src=\"./images/attention_compute_k.png\" alt=\"Computing the K matrix\" style=\"width: 800px;\">\n",
    "\n",
    "<img src=\"./images/attention_compute_k.png\" alt=\"Computing the V matrix\" style=\"width: 800px;\">\n",
    "\n",
    "We then multiply the **Queries** by the transpose of the **Keys** so that the query for each word calcualtes a similarity value with the keys for all of the words. **NOTE:** As seen in the illustration below, Masked Self-Attention calculates the values for all **Query/Key** pairs, but, ultimately, ignores values for when a token's **Query** comes before other token's **Keys**. For example, if the **Query** is for the first token **\\<SOS>**, then Masked Self-Attention will ignore the values calculated with **Keys** for **Let's** and **go**, because those tokens come after **\\<SOS>**.\n",
    "\n",
    "<img src=\"./images/attention_q_times_kt.png\" alt=\"Calculating the similarities between Queries and Keys\" style=\"width: 800px;\">\n",
    "\n",
    "The next step is to scale the similarity scores by the square root of the number of columns in the **Key** matrix, which represents the number of values used to represent each token. In this case, we scale by the square root of 2.\n",
    "\n",
    "<img src=\"./images/attention_scaling_scores.png\" alt=\"Scaling the similarities\" style=\"width: 800px;\">\n",
    "\n",
    "Now, if we were doing Masked Self-Attention, we would mask out the values we want to ignore by adding -infinity to them, as seen below. This step is the only difference between Self-Attention and Masked Self-Attention. \n",
    "\n",
    "<img src=\"./images/attention_masking.png\" alt=\"Masking out scaled similarities for Masked Self-Attention\" style=\"width: 800px;\">\n",
    "\n",
    "The next step is to applyt the **SoftMax()** function to each row in the scaled similarities. We'll do this first for the Self-Attention without a mask (below)...\n",
    "\n",
    "<img src=\"./images/attention_softmax.png\" alt=\"Applying the SoftMax() function to each row in the scaled similarity matrix\" style=\"width: 800px;\">\n",
    "\n",
    "...and we'll also do it for Masked Self-Attention (below).\n",
    "\n",
    "<img src=\"./images/attention_softmax_masked.png\" alt=\"Applying the SoftMax() function to each row in the masked scaled similarity matrix\" style=\"width: 800px;\">\n",
    "\n",
    "The SoftMax() function gives us percentages that the **Values** for each token should contribute to the attention score for a specific token. Thus, we can get the final attention scores by multiplying the percentages with the **Values** in matrix **V**. First, we'll do this with the un-masked percentages...\n",
    "\n",
    "<img src=\"./images/attention_final_scores.png\" alt=\"Calculating the final attention scores\" style=\"width: 800px;\">\n",
    "\n",
    "...and then we'll calculate the final Masked Self-Attention scores.\n",
    "\n",
    "<img src=\"./images/attention_final_scores_masked.png\" alt=\"Calculating the final attention scores\" style=\"width: 800px;\">\n",
    "\n",
    "# BAM!\n",
    "\n",
    "Now that we know how to calculate the differen types of attention, let's code the `Attention()` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3392130-cd25-4000-97bb-9612764c83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module): ## NOTE: We only need to inherit from L.LightningModule \n",
    "                            ##       in the class that puts all the pieces together.\n",
    "    def __init__(self, d_model=2):\n",
    "        ## d_model = the number of embedding values per token.\n",
    "        ##           In the transformer I used in the StatQuest: Transformer Neural Networks Clearly Explained!!!\n",
    "        ##           d_model=2, so that's what we'll use as a default for now.\n",
    "        ##           However, in \"Attention Is All You Need\" d_model=512\n",
    "\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ## Initialize the Weights (W) that we'll use to create the\n",
    "        ## query (q), key (k) and value (v) for each token\n",
    "        ## NOTE: Most implementations that I looked at include the bias terms\n",
    "        ##       but I didn't use them in my video (since they are not in the \n",
    "        ##       original Attention is All You Need paper).\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        \n",
    "        ## We'll keep track of which dimension specifies the rows in a matrix \n",
    "        ## and which specifies the columns. We're doing this because usually\n",
    "        ## people train with batches of data, and the first dimension, dimension 0\n",
    "        ## is used for the batch, dimension 1 is for rows and dimension 2 is for columns.\n",
    "        ## However, in this example, we are not using batches. However, by using\n",
    "        ## variables, we can easily change things if we wanted to use batches.\n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "\n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
    "        ## Create the Query, Key and Values using the encodings\n",
    "        ## associated with each token\n",
    "        ## For normal Self-Attention and Masked Self-Attention...\n",
    "        ##\n",
    "        ## encodings_for_q == encodings_for_k == encodings_for_v\n",
    "        ##\n",
    "        ## ...however, for Encoder-Decoder Attention, encodings_for_q comes from the Decoder\n",
    "        ## and encodings_for_k and encodings_for_v are different and come from the Encoder.\n",
    "        q = self.W_q(encodings_for_q)\n",
    "        k = self.W_k(encodings_for_k)\n",
    "        v = self.W_v(encodings_for_v)\n",
    "        \n",
    "        ## Compute attention scores\n",
    "        ## the equation is (q * k^T)/sqrt(d_model)\n",
    "        ## NOTE: It seems most people use \"reverse indexing\" for the dimensions when transposing k\n",
    "        ##       k.transpose(dim0, dim1) will transpose k by swapping dim0 and dim1\n",
    "        ##       In standard matrix notation, we would want to swap rows (dim=0) with columns (dim=1)\n",
    "        ##       If we have 3 dimensions, because of batching, and the batch was the first dimension\n",
    "        ##       And thus dims are defined batch = 0, rows = 1, columns = 2\n",
    "        ##       then dim0=-2 = 3 - 2 = 1. dim1=-1 = 3 - 1 = 2.\n",
    "        ##       Alternatively, we could put the batches in dim 3, and thus, dim 0 would still be rows\n",
    "        ##       and dim 1 would still be columns. I'm not sure why batches are put in dim 0...\n",
    "        ##\n",
    "        ##       Likewise, the q.size(-1) uses negative indexing to reverse to the number of columns in the query\n",
    "        ##       which tells us d_model. Alternatively, we could ust q.size(2) if we have batches in the first\n",
    "        ##       dimension or q.size(1) if we have batches in the 3rd dimension.\n",
    "        ##\n",
    "        ##       Since there are a bunch of ways to index things, I think the best thing to do is use\n",
    "        ##       variables \"row_dim\" and \"col_dim\" instead of numbers...\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "        scaled_sims = sims / torch.tensor(q.size(self.col_dim)**0.5)\n",
    "        \n",
    "        if mask is not None:\n",
    "            ## Here we are masking out things we don't want to pay attention to\n",
    "            ## (like the <PAD> (which is used when we have a batch of inputs sequences\n",
    "            ## and they are not all the exact same length... Because the batch is passed\n",
    "            ## in as a matrix, each input sequence has to have the same length, so we\n",
    "            ## add <PAD> to the shorter sequences so that they are all as long ast the\n",
    "            ## longest sequence.))\n",
    "            ##\n",
    "            ## We replace <PAD> and other things we wanted masked out\n",
    "            ## with a very large negative number (to approximate -infinity) so that the SoftMax() function\n",
    "            ## will give all masked elements an output value (or \"probability\") of 0.\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9) # I've also seen -1e20 and -9e15 used in masking\n",
    "        \n",
    "        ## Apply softmax to determine what percent of each token's value to\n",
    "        ## use in the final attention values.\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "        \n",
    "        ## Scale the values by their associated percentages and add them up.\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "        \n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291359b2-d509-43d3-aacc-648721690246",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32eab2-e3e3-4979-a0b2-749815841554",
   "metadata": {},
   "source": [
    "<a id=\"encoder\"></a>\n",
    "# The Encoder Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7976b261-9892-48f6-8fbc-1b83e4b06dad",
   "metadata": {},
   "source": [
    "Now that we have coded up the `PositionEncoding()` and `Attention()` classes, we're ready to put all the pieces together to build an `Encoder()` class, as seen in the figure below. \n",
    "\n",
    "<img src=\"./images/encoder_diagram.png\" alt=\"A diagram of an encoder\" style=\"width: 800px;\">\n",
    "\n",
    "\n",
    "A basic Encoder simply brings together...\n",
    "\n",
    "- Word Embedding\n",
    "- Position Encoding\n",
    "- Self-Attention\n",
    "- Residual Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737def7d-e0c7-4b73-b447-5bbcd798103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_tokens=4, d_model=2, max_len=3):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ## We set the seed so we start with the same random numbers each time.\n",
    "        ## This means, in theory, you should get the exact same results as me.\n",
    "        L.seed_everything(seed=42)\n",
    "\n",
    "        ## NOTE: In this simple example, we are just using a \"single layer\" encoder.\n",
    "        ##       If we wanted to have multiple layers of encoders, then we would\n",
    "        ##       take the output of one encoder module and use it as input to\n",
    "        ##       the next module.\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens,\n",
    "                               embedding_dim=d_model)     \n",
    "        \n",
    "        self.pe = PositionEncoding(d_model=d_model, \n",
    "                                   max_len=max_len)\n",
    "\n",
    "        self.self_attention = Attention(d_model=d_model)\n",
    "        ## NOTE: In this simple example, we are just doing plain old vanilla attention\n",
    "        ## If we wanted to do multi-head attention, we could\n",
    "        ## initailize more Attention objects like this...\n",
    "        ##\n",
    "        ## self.self_attention_2 = Attention(d_model=d_model)\n",
    "        ## self.self_attention_3 = Attention(d_model=d_model)\n",
    "        ##\n",
    "        ## If d_model=2, then using 3 self_attention objects would \n",
    "        ## result in d_model*3 = 6 self-attention values per token, \n",
    "        ## so we would need to initialize\n",
    "        ## a matrix of weights to reduce the dimension of the \n",
    "        ## self attention values back down to d_model...\n",
    "        ## \n",
    "        ## self.reduce_attention_dim = nn.Linear(in_features=(num_attention_heads*d_model), out_features=d_model)\n",
    "\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "\n",
    "        ## Get word embeddings\n",
    "        word_embeddings = self.we(token_ids)\n",
    "        \n",
    "        ## Add positional encoding to the word embeddings\n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "        \n",
    "        ## Calculate the self attention values\n",
    "        self_attention_values = self.self_attention(position_encoded, \n",
    "                                                    position_encoded, \n",
    "                                                    position_encoded)\n",
    "        ## NOTE: If we were doing multi-head attention, we would\n",
    "        ## calculate the self-attention values with the other attention objects\n",
    "        ## like this...\n",
    "        ##\n",
    "        ## self_attention_values_2 = self.self_attention_2(...)\n",
    "        ## self_attention_values 3 = self.self_attention_3(...)\n",
    "        ## \n",
    "        ## ...then we would concatenate all the self attention values...\n",
    "        ##\n",
    "        ## all_self_attention_values = torch.cat(self_attention_values_1, ...)\n",
    "        ##\n",
    "        ## ...and then run them through reduce_dim to get back to d_model values per token\n",
    "        ##\n",
    "        ## final_self_attention_values = self.reduce_attention_dim(all_self_attention_values)\n",
    "        \n",
    "        ## Add the position encoded values to the self attention values\n",
    "        ## To get the output values.\n",
    "        output_values = position_encoded + self_attention_values\n",
    "\n",
    "        return output_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb838cec-cfb0-460c-be9e-b1cb13057d03",
   "metadata": {},
   "source": [
    "# BAM!\n",
    "\n",
    "Now let's code the `Decoder()` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0082369c-0030-478d-892b-21e5e854d86d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ffd7e-6671-4153-8d47-4c3b74e61341",
   "metadata": {},
   "source": [
    "<a id=\"decoder\"></a>\n",
    "# The Decoder Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf2d3f7-41cd-4645-a105-6b9a924df22d",
   "metadata": {},
   "source": [
    "As we see in the figure below, the decoder class is almost the same as the encoder, execpt that it also includes Encoder-Decoder Attention, where the **Keys** and **Values** are created from the outputs from the Encoder, a fully connected layer so that we can have 5 outputs, one per word in the vocabulary, and a SoftMax() function to select the output token.\n",
    "\n",
    "<img src=\"./images/decoder_diagram.png\" alt=\"A diagram of an decoder\" style=\"width: 800px;\">\n",
    "\n",
    "A basic Decoder simply brings together...\n",
    "\n",
    "- Word Embedding\n",
    "- Position Encoding\n",
    "- Self-Attention\n",
    "- Residual Connections\n",
    "- Encoder-Decoder Attention\n",
    "- A fully connected layer\n",
    "- SoftMax - However, the loss function we are using `nn.CrossEntropyLoss()`, applies the SoftMax for us, so we will not include it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f286589-e933-46d2-aeda-600c74799357",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_tokens=4, d_model=2, max_len=3):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ## Just like in the encoder, we are setting the seed\n",
    "        ## so that you can get the same results as me.\n",
    "        ## NOTE: This time we are using seed=43, which\n",
    "        ## is different from what we did in the encoder.\n",
    "        ## We're using a different seed number so that we\n",
    "        ## will start out with different embedding values\n",
    "        L.seed_everything(seed=43)\n",
    "        \n",
    "        ## NOTE: Just like for the encoder, we are just using a \"single layer\" decoder.\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, \n",
    "                               embedding_dim=d_model)     \n",
    "        \n",
    "        self.pe = PositionEncoding(d_model=d_model, \n",
    "                                   max_len=max_len)\n",
    "\n",
    "        ## NOTE: Just like for the encoder, we are only using a single head for\n",
    "        ##       self-attention and encoder-decoder attention\n",
    "        self.self_attention = Attention(d_model=d_model)\n",
    "\n",
    "        self.enc_dec_attention = Attention(d_model=d_model)\n",
    "\n",
    "        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
    "        \n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "        \n",
    "        \n",
    "    def forward(self, token_ids, encoder_values):\n",
    "        \n",
    "        word_embeddings = self.we(token_ids)\n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "\n",
    "        ## For the decoder, we need to use \"masked self-attention\" so that \n",
    "        ## when we are training, the decoder can't cheat and look ahead at\n",
    "        ## what words come after the current word it is working on.\n",
    "        ## To create the mask we are creating a matrix where the lower triangle\n",
    "        ## is filled with 0, and everything above the diagonal is filled with 0s.\n",
    "        mask = torch.tril(torch.ones((token_ids.size(dim=self.row_dim), token_ids.size(dim=self.row_dim))))\n",
    "        ## We then replace the 0s above the diagonal, which represent the words\n",
    "        ## we want to be masked out, with \"True\", and replace the 1s in the lower\n",
    "        ## triangle, which represent the words we want to include when we calcualte\n",
    "        ## self-attention for a specific word in the output, with \"False\".\n",
    "        mask = mask == 0\n",
    "        \n",
    "        self_attention_values = self.self_attention(position_encoded, \n",
    "                                                    position_encoded, \n",
    "                                                    position_encoded, \n",
    "                                                    mask=mask)  \n",
    "        \n",
    "        residual_connection_values = position_encoded + self_attention_values\n",
    "        \n",
    "        enc_dec_attention_values = self.enc_dec_attention(residual_connection_values,\n",
    "                                                          encoder_values,\n",
    "                                                          encoder_values)\n",
    "        \n",
    "        residual_connection_values = enc_dec_attention_values + residual_connection_values\n",
    "        \n",
    "        \n",
    "        fc_layer_output = self.fc_layer(residual_connection_values)\n",
    "\n",
    "        ## NOTE: We are not passing the fc_layer_output to a SoftMax like in the illustration because\n",
    "        ## the loss function we're using, nn.CrossEntropyLoss(), will apply it for us.\n",
    "        \n",
    "        return fc_layer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae5288f-80dc-4ae9-96d5-55b951feeec7",
   "metadata": {},
   "source": [
    "# BAM!\n",
    "\n",
    "Now that we have coded up the `Encoder()` and `Decoder()` classes, all that's left is to code up a `Transformer()` that connects the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b932b757-e846-4166-b8ed-dafc36c1667c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146c5303-6195-4bc7-8ff5-178e8ed2b58f",
   "metadata": {},
   "source": [
    "<a id=\"transformer\"></a>\n",
    "# The Transformer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a095bdd-aa3c-436f-9759-03fd307ddc19",
   "metadata": {},
   "source": [
    "The `Transformer()` class simply connects the outputs from the Encoder to the Decoder, as seen in the figure below.\n",
    "\n",
    "<img src=\"./images/enc_dec_transformer.png\" alt=\"an encoder-decoder neural network\" style=\"width: 800px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e4dd3-a141-4df9-aaae-53f561e6defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(L.LightningModule):\n",
    "\n",
    "    def __init__(self, input_size, output_size, d_model=2, max_len=3):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(num_tokens=len(input_vocab), d_model=d_model, max_len=max_len)\n",
    "        self.decoder = Decoder(num_tokens=len(output_vocab), d_model=d_model, max_len=max_len)\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs, labels): \n",
    "        \n",
    "        encoder_values = self.encoder(inputs)\n",
    "        output_presoftmax = self.decoder(labels, encoder_values)\n",
    "        \n",
    "        return(output_presoftmax)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self): \n",
    "        \n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx): \n",
    "        \n",
    "        input_i, label_i = batch # collect input\n",
    "        \n",
    "        ## First, let's append the <SOS> token to tokens used as input to the Encoder...\n",
    "        input_tokens = torch.cat((torch.tensor([0]), input_i[0]))\n",
    "        \n",
    "        ## ...and to the tokens used as input to the decoder.\n",
    "        teacher_forcing = torch.cat((torch.tensor([0]), label_i[0]))\n",
    "        \n",
    "        ## Now let's add the <EOS> token to the end of the known output\n",
    "        expected_output = torch.cat((label_i[0], torch.tensor([4])))\n",
    "                \n",
    "        output_i = self.forward(input_tokens, teacher_forcing)\n",
    "        loss = self.loss(output_i, expected_output)\n",
    "                    \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2fe257-6b03-410f-bda5-78a75147a42b",
   "metadata": {},
   "source": [
    "# BAM!\n",
    "\n",
    "Now that we've built the `Transformer()` class, let's see if it works correctly without training. To use the transformer, we encode an input phrase, either **\\<SOS> let's go** or **\\<SOS> to go**, with the Encoder, and then pass the outputs to a Decoder. The Decoder itself is in a loop that will continue to create output until it creates the **\\<EOS>** (End of Sequence) token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d882fcd-77ec-4279-8e10-bc16fa067ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, a reminder of our input and output vocabularies...\n",
    "# input_vocab = {'<SOS>': 0, # Start\n",
    "#                'lets': 1,\n",
    "#                'to': 2,\n",
    "#                'go': 3}\n",
    "\n",
    "# output_vocab = {'<SOS>': 0, # Start\n",
    "#                 'ir': 1,\n",
    "#                 'vamos': 2,\n",
    "#                 'y': 3,\n",
    "#                 '<EOS>': 4} # End\n",
    "max_length = 3\n",
    "\n",
    "## Create a tranformer object...\n",
    "transformer = Transformer(len(input_vocab), len(output_vocab), d_model=2, max_len=max_length)\n",
    "\n",
    "## Encode the user input...\n",
    "encoder_values = transformer.encoder(torch.tensor([0, 1, 3])) # <SOS> let's go # Expecting: 0, 2, 4 = <SOS> vamos <EOS>\n",
    "# encoder_values = transformer.encoder(torch.tensor([0, 2, 3])) # <SOS> to go  # Expecting: 0, 1, 4 = <SOS> ir <EOS>\n",
    "    \n",
    "## Since we initialize the decoder with the <SOS> token, we\n",
    "## can consider that <SOS> to be the first predicted token\n",
    "predicted_ids = torch.tensor([0]) # set the first predicted token to <SOS> to initialize the decoder\n",
    "for i in range(max_length):\n",
    "    ## given the current predicted tokens and the encoded input, \n",
    "    ## predict the next token with the decoder\n",
    "    ## NOTE: \"prediction\" is the output from the fully connected layer,\n",
    "    ##      not a softmax() function. We could, if we wanted to,\n",
    "    ##      Run \"prediction\" through a softmax() function, but \n",
    "    ##      since we're going to select the item with the largest value\n",
    "    ##      we can just use argmax instead...\n",
    "    prediction = transformer.decoder(predicted_ids, encoder_values)\n",
    "\n",
    "    ## Now use argmax() to select the id of the predicted token\n",
    "    ## NOTE: The first time we call decoder(), with just the <SOS> token \n",
    "    ##       to initialize things, prediction\n",
    "    ##       will be a matrix with a single row of values, \n",
    "    ##       the output from the fully connected layer, like this...\n",
    "    ##\n",
    "    ##       tensor([[ 0.0417,  0.0945,  0.2714, -0.0105,  0.0902]]\n",
    "    ##       \n",
    "    ##       We then take the index for the element with the largest\n",
    "    ##       value (the 3rd element in this case, which is \"vamos\") and then\n",
    "    ##       loop around and call decoder() a second time, this time with two tokens: <SOS> vamos\n",
    "    ##       This will return a prediction for <SOS> and a prediction for vamos like this...\n",
    "    ##\n",
    "    ##       [[ 0.0417,  0.0945,  0.2714, -0.0105,  0.0902],\n",
    "    ##        [ 0.8693, -1.0257, -0.6939,  0.1791, -0.6546]]\n",
    "    ##\n",
    "    ##       NOTE: The first row is the same as the row that was returned earlier.\n",
    "    ##       Since we already figured out that the first row predicts vamos, we \n",
    "    ##       Only need to get the prediction from the second row. So, to make sure\n",
    "    ##       we always apply argmax() to the final row in the matrix, we index\n",
    "    ##       the final row with -1.\n",
    "    ##\n",
    "    ##       ALSO NOTE: If you're wondering why we need to make a prediction for\n",
    "    ##       <SOS> every time we call the decoder...We do this because\n",
    "    ##       the decoder has self-attention, so the prediciton made\n",
    "    ##       from \"vamos\" requires keys and values from <SOS>\n",
    "    ##       Could we optimize this and just store things in a table? Probably.\n",
    "    ##       But that's another project...\n",
    "    predicted_id = torch.tensor([torch.argmax(prediction[-1,:])])\n",
    "    ## add the predicted token id to the list of predicted ids.\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "        \n",
    "    if (predicted_id == 4): ## if the prediction is <EOS> then we are done.\n",
    "        break\n",
    "        \n",
    "print(\"\\npredicted_ids:\", predicted_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd22ac6-4d40-42ff-85cc-62c711fa3d7a",
   "metadata": {},
   "source": [
    "And, without training, the transformer predicts **\\<SOS> vamos \\<SOS> ir**, but we wanted it to predict **\\<SOS> vamos \\<EOS>** So, since the transformer didn't correctly translate the English phrases into Spanish, we'll have to train it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d51f3f-02c7-4d30-8e18-43be37996454",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982ca23f-0c46-4aa9-aebf-32e5bcedece6",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "# Train the Transformer!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88803f6-efc2-4307-bcb2-15e2994b6999",
   "metadata": {},
   "source": [
    "To train a transformer, we simply create an object from the `Transformer()` class..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e3404f-af9d-414a-9886-2c7138a8cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(len(input_vocab), len(output_vocab), d_model=2, max_len=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc78dd9-ca22-4c6a-bd0f-26540716620e",
   "metadata": {},
   "source": [
    "...and then create a Lightning `Trainer()` and train the transformer with the `dataloader` that we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96bd93c-31f4-4ef8-8c4f-fd33c3c20344",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = L.Trainer(max_epochs=30)\n",
    "trainer.fit(transformer, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acf9d1f-ac29-4316-88bb-5c6d0f3e04b6",
   "metadata": {},
   "source": [
    "# Double BAM!!!\n",
    "\n",
    "Now that we've trained the transformer, let's use it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b80a32f-ded6-4299-bf89-12c6bbd8ced2",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c040285c-c6de-4aac-b5b9-925e84f5e83f",
   "metadata": {},
   "source": [
    "<a id=\"use\"></a>\n",
    "# Use the Trained Transformer!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0666f428-eb0d-4c43-91d5-48c67cdcc5f8",
   "metadata": {},
   "source": [
    "To use the transformer that we just trained, we encode an input phrase, either **\\<SOS> let's go** or **\\<SOS> to go**, with the Encoder, and then pass the outputs to a Decoder. The Decoder itself is in a loop that will continue to create output until it creates the **\\<EOS>** (End of Sequence) token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717eaa4e-2249-46a1-b01f-9ed76beed0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, a reminder of our input and output vocabularies...\n",
    "# input_vocab = {'<SOS>': 0, # Start\n",
    "#                'lets': 1,\n",
    "#                'to': 2,\n",
    "#                'go': 3}\n",
    "\n",
    "# output_vocab = {'<SOS>': 0, # Start\n",
    "#                 'ir': 1,\n",
    "#                 'vamos': 2,\n",
    "#                 'y': 3,\n",
    "#                 '<EOS>': 4} # End\n",
    "\n",
    "max_length = 3\n",
    "row_dim = 0\n",
    "col_dim = 1\n",
    "\n",
    "## Encode the user input...\n",
    "encoder_values = transformer.encoder(torch.tensor([0, 1, 3])) # <SOS> let's go # Expecting: 0, 2, 4 = <SOS> vamos <EOS>\n",
    "# encoder_values = transformer.encoder(torch.tensor([0, 2, 3])) # <SOS> to go  # Expecting: 0, 1, 4 = <SOS> ir <EOS>\n",
    "    \n",
    "## Since we initialize the decoder with the <SOS> token, we\n",
    "## can consider that <SOS> to be the first predicted token\n",
    "predicted_ids = torch.tensor([0]) # set the first predicted token to <SOS> to initialize the decoder\n",
    "for i in range(max_length):\n",
    "    ## given the current predicted tokens and the encoded input, \n",
    "    ## predict the next token with the decoder\n",
    "    ## NOTE: \"prediction\" is the output from the fully connected layer,\n",
    "    ##      not a softmax() function. We could, if we wanted to,\n",
    "    ##      Run \"prediction\" through a softmax() function, but \n",
    "    ##      since we're going to select the item with the largest value\n",
    "    ##      we can just use argmax instead...\n",
    "    prediction = transformer.decoder(predicted_ids, encoder_values)\n",
    "\n",
    "    ## Use argmax() to select the id of the predicted token\n",
    "    predicted_id = torch.tensor([torch.argmax(prediction[-1,:])])\n",
    "    ## add the predicted token id to the list of predicted ids.\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "        \n",
    "    if (predicted_id == 4): # if the prediction is <EOS>, then we are done\n",
    "        break\n",
    "        \n",
    "print(\"\\npredicted_ids:\", predicted_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d63261b-a4dc-49e8-93db-84c6cdef47e7",
   "metadata": {},
   "source": [
    "And the output is **\\<SOS> vamos \\<EOS>**, which is exactly what we want.\n",
    "\n",
    "# TRIPLE BAM!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
