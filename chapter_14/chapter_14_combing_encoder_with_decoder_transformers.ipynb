{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIa-kJ-Frdm-"
   },
   "source": [
    "# The StatQuest Illustrated Guide to Neural Networks and AI\n",
    "## Chapter 14 - Encoder-Only Transformers!!!\n",
    "\n",
    "## **Summarizing Documents with Text Clustering and Topic Modeling**\n",
    "\n",
    "----\n",
    "\n",
    "This tutorial is from the book, **[The StatQuest Illustrated Guide to Neural Networks and AI](https://www.amazon.com/dp/B0DRS71QVQ)**.\n",
    "\n",
    "**NOTE:** This notebook is heavily based on the tutorial in Chapter 5 of *[Hands-On Large Language Models](https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/)* by [Jay Alammar](https://www.linkedin.com/in/jalammar/) and [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst/). This book is amazing, and I highly recommend purchasing it ASAP. Anyway, I changed a few things from their original code and descriptions, mainly by adding to what they already did.\n",
    "\n",
    "The idea here (illustrated below) is that we start with a bunch of documents about all kinds of things, and we want to cluster the documents and give each cluster a title. We'll use an Encoder-Only LLM, or Representative AI, to cluster the documents based on similar themes, topics, and meanings rather than just keywords or neighboring words (like we did with Word Embedding). Then, we'll use a Decoder-Only LLM, or Generative AI, to give each cluster an excellent title. And we'll do all of this without knowing anything about these documents in advance. So, this gives us a great tool for exploratory data analysis and dealing with vast archives that we otherwise would need help with.\n",
    "\n",
    "<img src=\"https://github.com/StatQuest/signa/blob/main/chapter_14/images/text_summarization_overview.png?raw=1\" alt=\"the curse of high dimensionality\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMdgQ5tw1jZ4"
   },
   "source": [
    "\n",
    "---\n",
    "        \n",
    "**NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
    "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJqb5blUxPYs"
   },
   "source": [
    "# Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxKTbfqRYnET"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "## NOTE: Installation might take a couple of minutes, so go get yourself a snack!\n",
    "## %%capture prevents this cell from printing a ton of STDERR stuff to the screen\n",
    "\n",
    "!pip install bertopic datasets datamapplot WordCloud\n",
    "## bertopic - BERTopic is a BERT based LLM that combines transformers and c-TF-IDF\n",
    "##            to create clusters of easily interpretable topics\n",
    "##            NOTE: By default, bertopic comes with sentence-transformers (aka SBERT), UMAP and HDBSCAN\n",
    "##                  because those, plus C-TF-IDF, form the standard BERTopic pipeline.\n",
    "## datasets - a huggingface package that makes it easy to access datasets stored on their site\n",
    "## datamapplot - creates nice looking plots of data maps\n",
    "## WordCloud - generates word clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMvfcUTmZj-t"
   },
   "source": [
    "**NOTE**: Make sure to **restart** the runtime after installing the above packages!\n",
    "\n",
    "To restart your session:\n",
    "  - In Google Colab, click on the **Runtime** menu and select **Restart Session** from the pulldown menu\n",
    "  - In a local jupyter notebook, click on the **Kernel** menu and select **Restart Kernel** from the pulldown menu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHIkQSBkV2Y3",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhc5_9LB8g3Q"
   },
   "source": [
    "# Load the data\n",
    "\n",
    "In this tutorial, we will use a [subset](https://huggingface.co/datasets/MaartenGr/arxiv_nlp) of the [arXiv Dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv) that was created by [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst/) specifically for the *[Hands-On Large Language Models](https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/)* book. The dataset contains 44,949 abstracts published between 1991 and 2024 from ArXiv’s Computation and Language section, aka cs.CL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmWfCW1a8g3R"
   },
   "outputs": [],
   "source": [
    "# Load the data directly from huggingface using their datasets module\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"maartengr/arxiv_nlp\")[\"train\"]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrfbyzaX8g3R"
   },
   "source": [
    "Now, pull out the abstracts and the titles..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FkKYq9S8g3R"
   },
   "outputs": [],
   "source": [
    "abstracts = dataset[\"Abstracts\"]\n",
    "titles = dataset[\"Titles\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2B4GQTjM8g3R"
   },
   "source": [
    "...and then print out the first 2 abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IiUiRyOp8g3R"
   },
   "outputs": [],
   "source": [
    "abstracts[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xk0YoARL8g3R"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcFT-b2QV3_r"
   },
   "source": [
    "# **Part 1: Text Clustering**\n",
    "\n",
    "There are many methods for text clustering, but the one we'll use in this tutorial has\n",
    "three steps:\n",
    "\n",
    "1. Convert the input documents to embeddings with an *embedding model*.\n",
    "    In this example, we'll use [SBERT](https://www.sbert.net/), also knowing as SentenceTransformers, to create the embeddings. SBERT is an Encoder-Only Transformer, which is a type of Representative AI.\n",
    "2. Reduce the dimensionality of embeddings with a *dimensionality reduction model*. In this example,\n",
    "   we'll use UMAP to reduce the dimensions of the model. To learn more about UMAP, see this [StatQuest](https://youtu.be/eN0wFzBA4Sc).\n",
    "3. Find groups of semantically similar documents with a *cluster model*. In this example, we'll use HDBSCAN, which is a hierarchical version of DBSCAN. To learn more about DBSCAN, check out the [StatQuest](https://youtu.be/RDZUdRSDOok) and [this great description of HDBSCAN specifics.](https://pberba.github.io/stats/2020/01/17/hdbscan/)\n",
    "\n",
    "We'll start by converting the abstracts into embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBpD9d2EV6HJ"
   },
   "source": [
    "## **Part 1, Step 1: Convert the abstracts into embeddings**\n",
    "\n",
    "The pre-trained SBERT model that we'll use to convert the abstracts into embeddings is the [gte-small model](https://huggingface.co/thenlper/gte-small) - the \"gte\" part stands for \"General Text Embeddings,\" and the \"small\" refers to its relative size; it only has 33 million parameters, whereas many other models have well over 100 million parameters (to see for yourself, you can compare models on HuggingFace's [Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard). **NOTE:** The gte-small model only accepts 512 tokens and any input that is longer than that is truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woTpVEpCV5kH"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# download the pre-trained SBERT model\n",
    "embedding_model = SentenceTransformer('thenlper/gte-small')\n",
    "\n",
    "## create the embeddings for the abstracts.\n",
    "embeddings = embedding_model.encode(abstracts, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbrzWSXn8g3S"
   },
   "source": [
    "Now that we've created the embeddings, we can verify that all 44,949 abstracts have 384 word embedding values associated with them by printing out the `.shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZwmECcNA519"
   },
   "outputs": [],
   "source": [
    "# Check the dimensions of the resulting embeddings\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_nsz_U7WEsw"
   },
   "source": [
    "## **Part 1, Step 2: Reduce the Dimensionality of Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGCQ3eyM8g3S"
   },
   "source": [
    "The model we used to create the embeddings, gte-small, is a general-purpose model designed to perform well given a wide variety of tasks, like classification and ranking, and, as a result, creates more embedding values than we need for clustering. Having too many embedding values, or dimensions, results in sparse data that can be difficult to cluster and is an example of [The Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). The illustration below shows the curse in action. On the left, we have 3 data points in a 2-dimensional graph. Each point is relatively far from the others, and thus, it's difficult to know how to best cluster them. However, in the center, we reduced the dimension to just the y-axis and, on the right side, we we see points A and B are relatively close to each other compared to point C. Thus, we'll put points A and B together in a cluster.\n",
    "\n",
    "<img src=\"https://github.com/StatQuest/signa/blob/main/chapter_14/images/curse_of_high_d.png?raw=1\" alt=\"the curse of high dimensionality\" width=\"800\" />\n",
    "\n",
    "One popular method for reducing dimensions is UMAP, which stands for Uniform Manifold Approximation and Projection, and we'll use it to reduce the dimensions from 384 to 5. **NOTE:** The distance metric that we are using is the Cosine Similarity. If you are not already familiar with the Cosine Similarity, check out the [StatQuest](https://youtu.be/e9U0QAFbfLI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXeo08d1WvOJ"
   },
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "# We reduce the input embeddings from 384 dimenions to 5 dimenions\n",
    "umap_model = UMAP(\n",
    "    n_components=5,\n",
    "    min_dist=0.0,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "reduced_embeddings_5 = umap_model.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3w2Uqj1C8g3S"
   },
   "outputs": [],
   "source": [
    "# Check the dimensions of the reduced embeddings\n",
    "reduced_embeddings_5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqzr_8LFWGld"
   },
   "source": [
    "## **Part 1, Step 3: Cluster the Reduced Embeddings**\n",
    "\n",
    "Now that we have reduced the number of dimensions (or values) of our embeddings from 384 to just 5, we can try to find clusters. In this example, we'll use HDBSCAN to find the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwq6qDydgWSP"
   },
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "\n",
    "## NOTE: The HDBSCAN call will generate a bunch of warnings.\n",
    "## If those bother you, you can run this code:\n",
    "# import os\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Find and label clusters\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=50, # each cluster must have >= 50 documents in it\n",
    "    metric='euclidean',  # clustering is based on euclidean distance\n",
    "    cluster_selection_method='eom' # eom = Excess of Mass and is the default clustering selection method\n",
    ").fit(reduced_embeddings_5)\n",
    "clusters = hdbscan_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sm5VwFy8g3S"
   },
   "outputs": [],
   "source": [
    "# Print out the number of cluseters\n",
    "len(set(clusters)) # set() removes duplicates - so we only count the unique cluster labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vi6ACUR4pU0f"
   },
   "source": [
    "Now that we have created the clusters, we can examine the first one to see if it makes sense. We do this by printing out the first three abstracts in the first cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiPK4m0rpJl7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Print first three documents in cluster 0\n",
    "cluster = 0\n",
    "for index in np.where(clusters==cluster)[0][:3]:\n",
    "    print(abstracts[index][:300] + \"... \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9izgGjMX1Go"
   },
   "source": [
    "### Draw a 2-d graph of the clusters\n",
    "\n",
    "Drawing a graph to visualize the clusters takes two steps. First, we reduce the original 384 embeddings to two dimensions so that we can visualize them and get a rough understanding of the generated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzr6xbTEYcyl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reduce 384-dimensional embeddings to 2 dimensions for easier visualization\n",
    "reduced_embeddings_2 = UMAP(\n",
    "    n_components=2,\n",
    "    min_dist=0.0,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ").fit_transform(embeddings)\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(reduced_embeddings_2, columns=[\"x\", \"y\"])\n",
    "df[\"title\"] = titles\n",
    "df[\"cluster\"] = [str(c) for c in clusters]\n",
    "\n",
    "# Select outliers and non-outliers (clusters)\n",
    "clusters_df = df.loc[df.cluster != \"-1\", :]\n",
    "outliers_df = df.loc[df.cluster == \"-1\", :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3bUq3HHyQcw"
   },
   "source": [
    "Now that we have 2-dimensional data, we can do the second step: plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nptZUr0mLWSN"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot outliers and non-outliers seperately\n",
    "plt.scatter(outliers_df.x, outliers_df.y, alpha=0.05, s=2, c=\"grey\")\n",
    "plt.scatter(\n",
    "    clusters_df.x, clusters_df.y, c=clusters_df.cluster.astype(int),\n",
    "    alpha=0.6, s=2, cmap='tab20b'\n",
    ")\n",
    "plt.axis('off')\n",
    "# plt.savefig(\"matplotlib.png\", dpi=300)  # Uncomment to save the graph as a .png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEE4EAA18g3S"
   },
   "source": [
    "**BAM!**\n",
    "We've got a graph that shows how the 44,949 abstracts cluster. It's a bit messy, and because we have so many clusters, the colors are reused for different clusters (this means the light green patch at the top is a distinct cluster from the light green patch close to the middle). However, it's still cool.\n",
    "\n",
    "Now we're done with the **Text Clustering** part of this tutorial. The next step, **Topic Modeling**, gives each cluster an easy to understand label based on the overall theme that each cluster represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-otFxRJW8g3S"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BgkDN8gWUUD"
   },
   "source": [
    "# **Part 2: Topic Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J27gpZsq8g3T"
   },
   "source": [
    "To generate labels for each cluster, we'll first generate keywords and then feed those keywords into a generative LLM to create the labels. We'll use **BERTopic** to coordinate both steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOKoHbdh8g3T"
   },
   "source": [
    "## **Part 2, Step 1: Generate keywords**\n",
    "\n",
    "We'll start by using **BERTopic** to do what we did in Part 1 (Text Clustering by first creating word embeddings, then reducing the dimensions of the embeddings, and then clustering). However, because we are using BERTopic this time, it will also extract keywords representing each cluster.\n",
    "The keywords are determined with a class-based variant of Term Frequency Inverse Document Frequency **c-TF-IDF**.\n",
    "\n",
    "To get **BERTopic** going, we pass it the embedding, UMAP, and DBSCAN models we created earlier, and fit it to the abstracts using the embeddings we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oySav_RqKvsS"
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# Train our model with our previously defined models\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    verbose=True\n",
    ").fit(abstracts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Popz6QTitaWB"
   },
   "source": [
    "Now that we have a topic model, we can print out the keywords it selected for each topic. **NOTE:** The first topic is labeled -1, which is the ID that DBSCAN gives to outliers. So, the very first row shows keywords for the outliers and isn't super interesting. However, the other rows are super interesting!\n",
    "\n",
    "- **Topic** is the topic ID number.\n",
    "- **Count** tells us how many documents are in that cluster, so that means we had 13,888 outliers that were not part of any particular cluster.\n",
    "- **Name** is the topic ID number appended to the first 4 keywords for a topic.\n",
    "- **Representation** lists the keywords for a topic.\n",
    "- **Representitive_Docs** lists the documents with the most occurrences of the topic's keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8vYjkscqUeX"
   },
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TNa-xEttfkf"
   },
   "source": [
    "To get the top 10 keywords per topic as well as their c-TF-IDF weights, we can use the `get_topic()` function. For example, for the first non-outlier topic..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPtrkto0wYyc"
   },
   "outputs": [],
   "source": [
    "topic_model.get_topic(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8_tWTkowTLX"
   },
   "source": [
    "We can use the `find_topics()` function to search for specific topics based on a search term. Let’s search for a topic about topic modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGzBPAt4pCfm"
   },
   "outputs": [],
   "source": [
    "topic_model.find_topics(\"topic modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwMgKD7OzFJs"
   },
   "source": [
    "We can also see if the BERTopic paper was also assigned to topic the same topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sp_WTxHazA-g"
   },
   "outputs": [],
   "source": [
    "topic_model.topics_[titles.index('BERTopic: Neural topic modeling with a class-based TF-IDF procedure')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5EZ3pvzzEpI"
   },
   "source": [
    "It is! **BAM!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITAXduKI8g3W"
   },
   "source": [
    "Now that we have keywords for each topic, we can use BERTopic to create a new, interactive 2-D graph of the text clusters that includes the topic labels and shows manuscript titles when the mouse moves over the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 767
    },
    "id": "rR6UXe1t5H8_",
    "outputId": "f3964771-fb30-440a-d646-302b587c2d86"
   },
   "outputs": [],
   "source": [
    "## Visualize topics and documents\n",
    "## NOTE: THIS WORKS ON GOOGLE COLAB BUT NOT ON RUNPOD...\n",
    "fig = topic_model.visualize_documents(\n",
    "    titles,\n",
    "    reduced_embeddings=reduced_embeddings_2,\n",
    "    width=1200,\n",
    "    hide_annotations=True\n",
    ")\n",
    "\n",
    "# Update fonts of legend for easier visualization\n",
    "fig.update_layout(font=dict(size=16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-8h5SnZmLe5"
   },
   "source": [
    "## **Part 2, Step 2: Using Generative AI to create topic labels**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yY6hAIvOo9zT"
   },
   "source": [
    "Now that we have the keywords, we'll add a Decoder-Only LLM, or Generative AI, to the **BERTopic** framework to create labels. However,\n",
    "first, we will duplicate our original topic model to easily compare the original keywords with the new labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usLIeMFbqM7y"
   },
   "outputs": [],
   "source": [
    "# Save original representations\n",
    "from copy import deepcopy\n",
    "original_topics = deepcopy(topic_model.topic_representations_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DodVj6a8g3W"
   },
   "source": [
    "We'll also create a helper function `compare_topics` that shows the original and the new topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dYLwbyEoeag"
   },
   "outputs": [],
   "source": [
    "def topics_vs_labels(original_topics, model, nr_topics=5):\n",
    "    \"\"\"Show the differences in topics and the new labels\"\"\"\n",
    "    df = pd.DataFrame(columns=[\"Topic ID\", \"Original Topics\", \"New Labels\"])\n",
    "    for topic in range(nr_topics):\n",
    "\n",
    "        # Extract top 5 words from the original topic\n",
    "        og_words = \" | \".join(list(zip(*original_topics[topic]))[0][:5])\n",
    "        new_words = model.get_topic(topic)[0][0]\n",
    "        df.loc[len(df)] = [topic, og_words, new_words]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPrZ4BPG8g3X"
   },
   "source": [
    "Now that we have the helper function, we'll generate labels from the keywords and abstracts using the **flan-t5-small** Decoder-Only LLM for Generative AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yisDTfcIrbq"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from bertopic.representation import TextGeneration\n",
    "\n",
    "prompt = \"\"\"I have a topic that contains the following documents:\n",
    "[DOCUMENTS]\n",
    "\n",
    "The topic is described by the following keywords: '[KEYWORDS]'.\n",
    "\n",
    "Based on the documents and keywords, what is this topic about?\"\"\"\n",
    "\n",
    "# Update our topic representations using Flan-T5\n",
    "generator = pipeline('text2text-generation', model='google/flan-t5-small', device=0)\n",
    "representation_model = TextGeneration(\n",
    "    generator, prompt=prompt, doc_length=50, tokenizer=\"whitespace\"\n",
    ")\n",
    "topic_model.update_topics(abstracts, representation_model=representation_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xj3Lrq0r8g3X"
   },
   "source": [
    "Now, let's compare the original keywords to the new labels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csvcjKHr8g3X"
   },
   "outputs": [],
   "source": [
    "# compare the original topics with the new labels\n",
    "topics_vs_labels(original_topics, topic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQPglbXq8g3X"
   },
   "source": [
    "Now, it's probably subjective, but to me, the new labels do a good job capturing the essence of the keywords. So I say **BAM!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjU4TFgf8g3X"
   },
   "source": [
    "### **Draw an awesome 2-D graph of the documents with the new labels**\n",
    "\n",
    "Now that we have labels for each cluster, let's draw a very cool looking graph that shows the document clusters with the new labels. This still uses the **BERTopic** framework that we've been working with, which uses the `datamapplot` module to draw the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36c3UUvXZ8L-"
   },
   "outputs": [],
   "source": [
    "# Visualize documents with labeled topics\n",
    "fig = topic_model.visualize_document_datamap(\n",
    "    titles,\n",
    "    topics=list(range(20)),\n",
    "    reduced_embeddings=reduced_embeddings_2,\n",
    "    width=1200\n",
    "    # label_font_size=11,\n",
    "    # label_wrap_width=20,\n",
    "    # use_medoids=True,\n",
    ")\n",
    "plt.savefig(\"datamapplot.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHsUs8DR8g3X"
   },
   "source": [
    "What a cool looking graph! And its got awesome easy to understand labels!!\n",
    "\n",
    "# **TRIPLE BAM!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fj9Z47wkxTzl"
   },
   "source": [
    "\n",
    "## **BONUS**: WordCloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdq9LzkxzxzT"
   },
   "source": [
    "First, we need to ensure that each topic is described by a bit more words than just 10, as that would make for a much more interesting WordCloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qT2IKjyfxkkV"
   },
   "outputs": [],
   "source": [
    "topic_model.update_topics(abstracts, top_n_words=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3NIkt0Hz4AJ"
   },
   "source": [
    "Then, we can run the following code to generate the WordCloud for our topic modeling topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFSNG30jxWGo"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_wordcloud(model, topic):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    text = {word: value for word, value in model.get_topic(topic)}\n",
    "    wc = WordCloud(background_color=\"white\", max_words=1000, width=1600, height=800)\n",
    "    wc.generate_from_frequencies(text)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Show wordcloud\n",
    "create_wordcloud(topic_model, topic=17)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
