{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "453a4f64-f611-4b3f-b452-2eee89913606",
   "metadata": {},
   "source": [
    "# The StatQuest Illustrated Guide to Neural Networks and AI\n",
    "## Chapter 10 - Seq2Seq and Encoder-Decoder Models with LSTMs\n",
    "\n",
    "Copyright 2024, Joshua Starmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0164aa71-2955-426d-b7af-8571a26c1f66",
   "metadata": {},
   "source": [
    "This tutorial is from the book, **[The StatQuest Illustrated Guide to Neural Networks and AI](https://www.amazon.com/dp/B0DRS71QVQ)**.\n",
    "\n",
    "In this notebook, we will build and train a Seq2Seq or Encoder-Deocder model with 2 layers of LSTMs, each layer with 2 stacks of LSTMs as seen in the picture below.\n",
    "\n",
    "<img src=\"./images/full_model.png\" alt=\"an encoder-decoder model with 2 layers of LSTMs, each layer with 2 stacks of LSTMs\" style=\"width: 800px;\">\n",
    "\n",
    "In this tutorial, you will...\n",
    "\n",
    "#### NOTE:\n",
    "This tutorial assumes that you have read through the chapter on **Seq2Seq and Encoder-Decoder Models** in **The StatQuest Illustrated Guide to Neural Networks and AI**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058c8f08-2aba-4947-b197-2ec6add52aaa",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa40bf7-32ee-4278-91ba-b78a222c68ed",
   "metadata": {},
   "source": [
    "# Import the modules that will do all the work\n",
    "\n",
    "The very first thing we need to do is load a bunch of Python modules. Python itself is just a basic programming language. These modules give us extra functionality to create and train a Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b27bdee-8e09-419d-ba2d-b7c4c32caebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# %%capture prevents this cell from printing a ton of STDERR stuff to the screen\n",
    "\n",
    "## First, check to see if lightning is installed, if not, install it.\n",
    "import pip\n",
    "try:\n",
    "  __import__(\"lightning\")\n",
    "except ImportError:\n",
    "  pip.main(['install', \"lightning\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5f3628-5b95-4302-b365-56fc93b63d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch ## torch let's us create tensors and also provides helper functions\n",
    "import torch.nn as nn ## torch.nn gives us nn.Module(), nn.Embedding() and nn.Linear()\n",
    "import torch.nn.functional as F # This gives us the softmax() and argmax()\n",
    "from torch.optim import Adam ## We will use the Adam optimizer, which is, essentially, \n",
    "                             ## a slightly less stochastic version of stochastic gradient descent.\n",
    "from torch.utils.data import TensorDataset, DataLoader ## We'll store our data in DataLoaders\n",
    "\n",
    "import lightning as L ## Lightning makes it easier to write, optimize and scale our code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007fea2-787e-433b-85c8-3ab21d6e117d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3eadcd-d980-4bf1-a713-2304695235a6",
   "metadata": {},
   "source": [
    "# Create the datasets that we will use for training Encoder-Decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf6e48-e2b0-46fe-8df3-43526bc1a43a",
   "metadata": {},
   "source": [
    "To make the model at least a little bit interesting, we will translate two english phrases, **Let's go** and **to go** into spanish. **Let's go** should translate to **vamos \\<EOS\\>** and **to go** should translate to **ir \\<EOS\\>**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea923c1e-2527-4bee-bffa-dcf9a2c4c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first, we create a dictionary that maps vocabulary tokens to id numbers...\n",
    "english_token_to_id = {'lets': 0,\n",
    "                       'to': 1,\n",
    "                       'go': 2,\n",
    "                       '<EOS>': 3 ## <EOS> = end of sequence\n",
    "                      }\n",
    "## ...then we create a dictionary that maps the ids to tokens. This will help us interpret the output.\n",
    "## We use the \"map()\" function to apply the \"reversed()\" function to each tuple (i.e. ('lets', 0)) stored\n",
    "## in the token_to_id dictionary. We then use dict() to make a new dictionary from the\n",
    "## reversed tuples.\n",
    "english_id_to_token = dict(map(reversed, english_token_to_id.items()))\n",
    "\n",
    "spanish_token_to_id = {'ir': 0,\n",
    "                       'vamos': 1,\n",
    "                       'y': 2,\n",
    "                       '<EOS>': 3}\n",
    "spanish_id_to_token = dict(map(reversed, spanish_token_to_id.items()))\n",
    "\n",
    "inputs = torch.tensor([[english_token_to_id[\"lets\"],\n",
    "                        english_token_to_id[\"go\"]],\n",
    "\n",
    "                       [english_token_to_id[\"to\"],\n",
    "                        english_token_to_id[\"go\"]]])\n",
    "\n",
    "labels = torch.tensor([[spanish_token_to_id[\"vamos\"],\n",
    "                        spanish_token_to_id[\"<EOS>\"]],\n",
    "\n",
    "                       [spanish_token_to_id[\"ir\"],\n",
    "                        spanish_token_to_id[\"<EOS>\"]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26db742d-9789-43d9-905a-1d5be57cc58e",
   "metadata": {},
   "source": [
    "Now that we have created the data that we want to train the embeddings with, we'll store it in a `DataLoader`. Since our dataset is so small, using a `DataLoader` is a little bit of an overkill, but it it's easy to do, and it will allow us to easily scale up to a much larger vocabulary when the time comes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39473af-725d-4ac9-bdc6-8927638daaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's package everything up into a DataLoader...\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7808876d-0c42-4adf-b576-43c29167ed28",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4230f3-17e4-4923-b08c-461b092dfcfc",
   "metadata": {},
   "source": [
    "# Build and Train a Seq2Seq/Encoder-Decoder Model from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff9373-167f-4037-bea1-345183880af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(L.LightningModule):\n",
    "\n",
    "    def __init__(self, max_len=2):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.max_output_length = max_len\n",
    "        \n",
    "        L.seed_everything(seed=420)\n",
    "                \n",
    "        #################################\n",
    "        ##\n",
    "        ## ENCODING\n",
    "        ##\n",
    "        #################################\n",
    "        self.encoder_we = nn.Embedding(num_embeddings=4, # num_embeddings = # of words in input vocabulary\n",
    "                                       embedding_dim=2)  # embedding_dim = 2 numbers per embedding\n",
    "        \n",
    "        self.encoder_lstm = nn.LSTM(input_size=2, # input_size = number of inputs (2 numbers per word)\n",
    "                                    hidden_size=2,# hidden_size = number of outputs (2 per word per layer) \n",
    "                                    num_layers=2) # num_layers = how many lstm's to stack\n",
    "                                                  #          If there are 2 layers, then the short term memory from the \n",
    "                                                  #          first layer is used as input to the second layer\n",
    "                \n",
    "        #################################\n",
    "        ##\n",
    "        ## DECODING\n",
    "        ##\n",
    "        #################################\n",
    "        self.decoder_we = nn.Embedding(num_embeddings=4,\n",
    "                                       embedding_dim=2)\n",
    "                \n",
    "        self.decoder_lstm = nn.LSTM(input_size=2,\n",
    "                                    hidden_size=2,\n",
    "                                    num_layers=2)\n",
    "        \n",
    "        self.output_fc = nn.Linear(in_features=2,  # in_features = # of outputs per LSTM\n",
    "                                   out_features=4) # out_features = # of words in the output vocabulary\n",
    "        \n",
    "        #################################\n",
    "        ##\n",
    "        ## Training\n",
    "        ##\n",
    "        #################################\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    \n",
    "    def forward(self, input, output=None): \n",
    "         \n",
    "        #################################\n",
    "        ##\n",
    "        ## ENCODING\n",
    "        ##\n",
    "        #################################\n",
    "        ## first, use the encoder stage to create an intermediate encoding of the input text\n",
    "        encoder_embeddings = self.encoder_we(input)\n",
    "        encoder_lstm_output, (encoder_lstm_hidden, encoder_lstm_cell) = self.encoder_lstm(encoder_embeddings)\n",
    "\n",
    "        #################################\n",
    "        ##\n",
    "        ## DECODING\n",
    "        ##\n",
    "        #################################\n",
    "        ## We start by initializing the decoder with the <EOS> token...\n",
    "        decoder_token_id = torch.tensor([spanish_token_to_id[\"<EOS>\"]])\n",
    "        decoder_embeddings = self.decoder_we(decoder_token_id)\n",
    "\n",
    "        decoder_lstm_output, (decoder_lstm_hidden, decoder_lstm_cell) = self.decoder_lstm(decoder_embeddings,\n",
    "                                                                                          (encoder_lstm_hidden,\n",
    "                                                                                           encoder_lstm_cell))\n",
    "\n",
    "        output_values = self.output_fc(decoder_lstm_output)\n",
    "        outputs = output_values\n",
    "        \n",
    "        predicted_id = torch.tensor([torch.argmax(output_values)])\n",
    "        predicted_ids = predicted_id\n",
    "        \n",
    "        for i in range(1, self.max_output_length):\n",
    "\n",
    "            if (output == None): # using the model...\n",
    "                if (predicted_id == spanish_token_to_id[\"<EOS>\"]): # if the prediction is <EOS>, then we are done\n",
    "                    break\n",
    "                decoder_embeddings = self.decoder_we(predicted_id)\n",
    "            else:\n",
    "                ## run this when training the model\n",
    "                decoder_embeddings = self.decoder_we(torch.tensor([output[i-1]]))\n",
    "            \n",
    "            decoder_lstm_output, (decoder_lstm_hidden, decoder_lstm_cell) = self.decoder_lstm(decoder_embeddings,\n",
    "                                                                                              (decoder_lstm_hidden,\n",
    "                                                                                               decoder_lstm_cell))\n",
    "            \n",
    "            output_values = self.output_fc(decoder_lstm_output)\n",
    "            outputs = torch.cat((outputs, output_values), 0)\n",
    "            predicted_id = torch.tensor([torch.argmax(output_values)])\n",
    "            predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "        \n",
    "        return(outputs)\n",
    "       \n",
    "        \n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        return Adam(self.parameters(), lr=0.1) ## NOTE: Setting the learning rate to 0.1 trains way faster than\n",
    "                                               ## using the default learning rate, lr=0.001\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.        \n",
    "        input_tokens, labels = batch # collect input\n",
    "        output = self.forward(input_tokens[0], labels[0]) # run input through the neural network\n",
    "        loss = self.loss(output, labels[0]) ## self.loss = cross entropy\n",
    "        ###################\n",
    "        ##\n",
    "        ## Logging the loss\n",
    "        ##\n",
    "        ###################\n",
    "        # self.log(\"train_loss\", loss)\n",
    "                    \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f16ab35-a57b-42e0-a5df-d49145ff9c36",
   "metadata": {},
   "source": [
    "Now that we have created the `seq2seq()` class, let's just run the phrase **Let's go** through it to see what it gets translated into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fe24ca-9a43-43ce-9f31-3be2de14a6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = seq2seq()\n",
    "outputs = model.forward(input=torch.tensor([english_token_to_id[\"lets\"],\n",
    "                                            english_token_to_id[\"go\"]]), ## translate \"lets go\", we should get \"vamos <EOS>\"\n",
    "                        output=None)\n",
    "\n",
    "print(\"Translated text:\")\n",
    "predicted_ids = torch.argmax(outputs, dim=1)\n",
    "for id in predicted_ids: \n",
    "    print(\"\\t\", spanish_id_to_token[id.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414beac8-0122-4194-ab14-0cf80f0b4cf1",
   "metadata": {},
   "source": [
    "And we see that **Let's go** was translated to **\\<EOS\\>** instead of what we wanted, which was **vamos \\<EOS\\>**. So let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178aaa49-553a-4e41-96ef-b5255f09638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(max_epochs=40, accelerator=\"cpu\")\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422747c8-a312-444a-adef-2fd471f8ee3f",
   "metadata": {},
   "source": [
    "Now let's see if the model correctly translates **Let's go** into **vamos \\<EOS\\>**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76543606-366c-4fff-9a4d-b7f2cacb8392",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.forward(input=torch.tensor([english_token_to_id[\"lets\"],\n",
    "                                            english_token_to_id[\"go\"]]), ## translate \"lets go\", we should get \"vamos <EOS>\"\n",
    "                        output=None)\n",
    "\n",
    "print(\"Translated text:\")\n",
    "predicted_ids = torch.argmax(outputs, dim=1)\n",
    "for id in predicted_ids: \n",
    "    print(\"\\t\", spanish_id_to_token[id.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366e2428-056b-4cdd-90fd-d873a33fffb9",
   "metadata": {},
   "source": [
    "...and it does! \n",
    "\n",
    "### BAM!\n",
    "\n",
    "Now let's see if the model correctly translates **to go** to **ir \\<EOS\\>**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4fa33c-db3b-4973-8871-1cfaf647ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.forward(input=torch.tensor([english_token_to_id[\"to\"],\n",
    "                                            english_token_to_id[\"go\"]]), ## translate \"lets go\", we should get \"vamos <EOS>\"\n",
    "                        output=None)\n",
    "\n",
    "print(\"Translated text:\")\n",
    "predicted_ids = torch.argmax(outputs, dim=1)\n",
    "for id in predicted_ids: \n",
    "    print(\"\\t\", spanish_id_to_token[id.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4338216b-6404-4ac5-8b43-62cb3bce2786",
   "metadata": {},
   "source": [
    "...and it does!\n",
    "\n",
    "## DOUBLE BAM!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de7a059-86bb-49df-8110-fe296f5f2346",
   "metadata": {},
   "source": [
    "Now that we have model that works, let's just see count the number of parameters we had to train by counting the number of parameters where `requires_grad` was set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea1a19d-ff52-40e9-8dd5-2f6b4574c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## count the number of parameters...\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Total number of trainable parameters:\", total_trainable_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e2d3ce-2070-415a-9d62-769022b2f97a",
   "metadata": {},
   "source": [
    "So this model has 220 parameters. Compared to modern models used in practical situations, that's not very many. However, we could still spare us the agony of having to train this model whenever we wanted to use it by saving the trained parameters to a file and then loading them when needed. So let's do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cbd775-5471-42d7-9893-d973c41d0cb1",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a057e20d-f22e-4fb4-9b60-10fa96cbd648",
   "metadata": {},
   "source": [
    "# Saving and loading the trained model weights..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f06308-7d9e-4c3f-aaeb-9b27302e5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, save the weights...\n",
    "trainer.save_checkpoint(\"seq2seq_en2es_220_trained.ckpt\") ## NOTE: You can specify a path as part of the filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509d6b0e-6c4d-4e5d-a7f6-edbe9ad18c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's create a new model and load in the saved weights...\n",
    "new_model = seq2seq.load_from_checkpoint(\"seq2seq_en2es_220_trained.ckpt\")\n",
    "\n",
    "outputs = new_model.forward(input=torch.tensor([english_token_to_id[\"lets\"],\n",
    "                                                english_token_to_id[\"go\"]]),\n",
    "                            output=None)\n",
    "\n",
    "print(\"Translated text:\")\n",
    "predicted_ids = torch.argmax(outputs, dim=1)\n",
    "for id in predicted_ids: \n",
    "    print(\"\\t\", spanish_id_to_token[id.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85f8334-cd9e-460b-9fa0-3f1cf20865a1",
   "metadata": {},
   "source": [
    "# TRIPLE BAM!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
